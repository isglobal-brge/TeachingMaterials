<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Regresión lineal | Aprendizaje Automático 1</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Regresión lineal | Aprendizaje Automático 1" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Regresión lineal | Aprendizaje Automático 1" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

<meta name="author" content="Juan R González" />


<meta name="date" content="2020-09-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="tidyverse.html"/>

<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Aprendizaje Automático 1</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Inroducción</a></li>
<li class="chapter" data-level="2" data-path="tidyverse.html"><a href="tidyverse.html"><i class="fa fa-check"></i><b>2</b> Tidyverse</a>
<ul>
<li class="chapter" data-level="2.1" data-path="tidyverse.html"><a href="tidyverse.html#introducción"><i class="fa fa-check"></i><b>2.1</b> Introducción</a></li>
<li class="chapter" data-level="2.2" data-path="tidyverse.html"><a href="tidyverse.html#instalación"><i class="fa fa-check"></i><b>2.2</b> Instalación</a></li>
<li class="chapter" data-level="2.3" data-path="tidyverse.html"><a href="tidyverse.html#librerías-básicas"><i class="fa fa-check"></i><b>2.3</b> Librerías básicas</a></li>
<li class="chapter" data-level="2.4" data-path="tidyverse.html"><a href="tidyverse.html#manejo-de-datos"><i class="fa fa-check"></i><b>2.4</b> Manejo de datos</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="tidyverse.html"><a href="tidyverse.html#tibbles"><i class="fa fa-check"></i><b>2.4.1</b> Tibbles</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="tidyverse.html"><a href="tidyverse.html#exercises-tibbles"><i class="fa fa-check"></i><b>2.5</b> Exercises (tibbles)</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="tidyverse.html"><a href="tidyverse.html#importar-datos"><i class="fa fa-check"></i><b>2.5.1</b> Importar datos</a></li>
<li class="chapter" data-level="2.5.2" data-path="tidyverse.html"><a href="tidyverse.html#transformación-de-datos"><i class="fa fa-check"></i><b>2.5.2</b> Transformación de datos</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="tidyverse.html"><a href="tidyverse.html#dlpyr-basics"><i class="fa fa-check"></i><b>2.6</b> dlpyr basics</a></li>
<li class="chapter" data-level="2.7" data-path="tidyverse.html"><a href="tidyverse.html#filter-rows"><i class="fa fa-check"></i><b>2.7</b> Filter rows</a></li>
<li class="chapter" data-level="2.8" data-path="tidyverse.html"><a href="tidyverse.html#logical-filtering"><i class="fa fa-check"></i><b>2.8</b> Logical filtering</a></li>
<li class="chapter" data-level="2.9" data-path="tidyverse.html"><a href="tidyverse.html#arrange-rows"><i class="fa fa-check"></i><b>2.9</b> Arrange rows</a></li>
<li class="chapter" data-level="2.10" data-path="tidyverse.html"><a href="tidyverse.html#select-columns"><i class="fa fa-check"></i><b>2.10</b> Select columns</a></li>
<li class="chapter" data-level="2.11" data-path="tidyverse.html"><a href="tidyverse.html#add-new-variables"><i class="fa fa-check"></i><b>2.11</b> Add new variables</a></li>
<li class="chapter" data-level="2.12" data-path="tidyverse.html"><a href="tidyverse.html#grouped-summaries"><i class="fa fa-check"></i><b>2.12</b> Grouped summaries</a></li>
<li class="chapter" data-level="2.13" data-path="tidyverse.html"><a href="tidyverse.html#the-pipe"><i class="fa fa-check"></i><b>2.13</b> The pipe <code>%&gt;%</code></a></li>
<li class="chapter" data-level="2.14" data-path="tidyverse.html"><a href="tidyverse.html#group-by-different-variables"><i class="fa fa-check"></i><b>2.14</b> Group by different variables</a></li>
<li class="chapter" data-level="2.15" data-path="tidyverse.html"><a href="tidyverse.html#useful-summary-functions"><i class="fa fa-check"></i><b>2.15</b> Useful summary functions</a></li>
<li class="chapter" data-level="2.16" data-path="tidyverse.html"><a href="tidyverse.html#exercises-data-transform"><i class="fa fa-check"></i><b>2.16</b> Exercises (data transform)</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="regresión-lineal.html"><a href="regresión-lineal.html"><i class="fa fa-check"></i><b>3</b> Regresión lineal</a>
<ul>
<li class="chapter" data-level="3.1" data-path="regresión-lineal.html"><a href="regresión-lineal.html#preliminares"><i class="fa fa-check"></i><b>3.1</b> Preliminares</a></li>
<li class="chapter" data-level="3.2" data-path="regresión-lineal.html"><a href="regresión-lineal.html#conceptos-básicos"><i class="fa fa-check"></i><b>3.2</b> Conceptos básicos</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="regresión-lineal.html"><a href="regresión-lineal.html#modelo-lineal-simple"><i class="fa fa-check"></i><b>3.2.1</b> Modelo lineal simple</a></li>
<li class="chapter" data-level="3.2.2" data-path="regresión-lineal.html"><a href="regresión-lineal.html#regresión-lineal-multivariante"><i class="fa fa-check"></i><b>3.2.2</b> Regresión lineal multivariante</a></li>
<li class="chapter" data-level="3.2.3" data-path="regresión-lineal.html"><a href="regresión-lineal.html#incertidumbre"><i class="fa fa-check"></i><b>3.2.3</b> Incertidumbre</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="regresión-lineal.html"><a href="regresión-lineal.html#ajuste-de-un-modelo-lineal"><i class="fa fa-check"></i><b>3.3</b> Ajuste de un modelo lineal</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="regresión-lineal.html"><a href="regresión-lineal.html#residuos"><i class="fa fa-check"></i><b>3.3.1</b> Residuos</a></li>
<li class="chapter" data-level="3.3.2" data-path="regresión-lineal.html"><a href="regresión-lineal.html#coeficientes-de-interpretación"><i class="fa fa-check"></i><b>3.3.2</b> Coeficientes de interpretación</a></li>
<li class="chapter" data-level="3.3.3" data-path="regresión-lineal.html"><a href="regresión-lineal.html#interacciones"><i class="fa fa-check"></i><b>3.3.3</b> Interacciones</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="regresión-lineal.html"><a href="regresión-lineal.html#estimación-por-mínimos-cuadrados"><i class="fa fa-check"></i><b>3.4</b> Estimación por mínimos cuadrados</a></li>
<li class="chapter" data-level="3.5" data-path="regresión-lineal.html"><a href="regresión-lineal.html#medidas-adicionales-de-ajuste-del-modelo"><i class="fa fa-check"></i><b>3.5</b> Medidas adicionales de ajuste del modelo</a></li>
<li class="chapter" data-level="3.6" data-path="regresión-lineal.html"><a href="regresión-lineal.html#sesgo-variación-sobreajuste"><i class="fa fa-check"></i><b>3.6</b> Sesgo, variación, sobreajuste</a></li>
<li class="chapter" data-level="3.7" data-path="regresión-lineal.html"><a href="regresión-lineal.html#regresión-como-estimación-de-una-media-condicional"><i class="fa fa-check"></i><b>3.7</b> Regresión como estimación de una media condicional</a></li>
<li class="chapter" data-level="3.8" data-path="regresión-lineal.html"><a href="regresión-lineal.html#la-función-de-regresión"><i class="fa fa-check"></i><b>3.8</b> La función de regresión</a></li>
<li class="chapter" data-level="3.9" data-path="regresión-lineal.html"><a href="regresión-lineal.html#estimación-no-paramétrica-de-la-función-de-regresión-regresión-knn"><i class="fa fa-check"></i><b>3.9</b> Estimación no paramétrica de la función de regresión: regresión KNN</a></li>
<li class="chapter" data-level="3.10" data-path="regresión-lineal.html"><a href="regresión-lineal.html#estimación-paramétrica-de-la-función-de-regresión-regresión-lineal"><i class="fa fa-check"></i><b>3.10</b> Estimación paramétrica de la función de regresión: regresión lineal</a></li>
<li class="chapter" data-level="3.11" data-path="regresión-lineal.html"><a href="regresión-lineal.html#predicción-usando-el-objeto-del-modelo"><i class="fa fa-check"></i><b>3.11</b> Predicción usando el objeto del modelo</a></li>
<li class="chapter" data-level="3.12" data-path="regresión-lineal.html"><a href="regresión-lineal.html#inferencia-en-el-contexto-de-regresión"><i class="fa fa-check"></i><b>3.12</b> Inferencia en el contexto de regresión</a></li>
<li class="chapter" data-level="3.13" data-path="regresión-lineal.html"><a href="regresión-lineal.html#asunciones-de-un-modelo-de-regresión"><i class="fa fa-check"></i><b>3.13</b> Asunciones de un modelo de regresión</a></li>
<li class="chapter" data-level="3.14" data-path="regresión-lineal.html"><a href="regresión-lineal.html#ejemplos-adicionales-de-interpretación-de-modelos"><i class="fa fa-check"></i><b>3.14</b> Ejemplos adicionales de interpretación de modelos</a>
<ul>
<li class="chapter" data-level="3.14.1" data-path="regresión-lineal.html"><a href="regresión-lineal.html#interpretación-del-intercept-y-los-coeficientes-beta-para-un-modelo-con-predictores-continuos"><i class="fa fa-check"></i><b>3.14.1</b> Interpretación del <em>intercept</em> y los coeficientes <span class="math inline">\(\beta\)</span> para un modelo con predictores continuos</a></li>
<li class="chapter" data-level="3.14.2" data-path="regresión-lineal.html"><a href="regresión-lineal.html#interpretación-del-intercept-y-los-coeficientes-beta-para-un-modelo-con-predictores-binarios-y-continuos"><i class="fa fa-check"></i><b>3.14.2</b> Interpretación del <em>intercept</em> y los coeficientes <span class="math inline">\(\beta\)</span> para un modelo con predictores binarios y continuos</a></li>
<li class="chapter" data-level="3.14.3" data-path="regresión-lineal.html"><a href="regresión-lineal.html#interpretación-del-intercept-y-los-coeficientes-beta-para-un-modelo-con-predictores-binarios-y-continuos-con-interacciones"><i class="fa fa-check"></i><b>3.14.3</b> Interpretación del <em>intercept</em> y los coeficientes <span class="math inline">\(\beta\)</span> para un modelo con predictores binarios y continuos, con interacciones</a></li>
<li class="chapter" data-level="3.14.4" data-path="regresión-lineal.html"><a href="regresión-lineal.html#interpretación-del-intercept-y-los-coeficientes-beta-para-un-modelo-con-predictores-continuos-con-interacciones"><i class="fa fa-check"></i><b>3.14.4</b> Interpretación del <em>intercept</em> y los coeficientes <span class="math inline">\(\beta\)</span> para un modelo con predictores continuos, con interacciones</a></li>
</ul></li>
<li class="chapter" data-level="3.15" data-path="regresión-lineal.html"><a href="regresión-lineal.html#centrado-y-escalado"><i class="fa fa-check"></i><b>3.15</b> Centrado y escalado</a></li>
<li class="chapter" data-level="3.16" data-path="tidyverse.html"><a href="tidyverse.html#transformación-de-datos"><i class="fa fa-check"></i><b>3.16</b> Transformación de datos</a></li>
<li class="chapter" data-level="3.17" data-path="regresión-lineal.html"><a href="regresión-lineal.html#colinealidad"><i class="fa fa-check"></i><b>3.17</b> Colinealidad</a></li>
<li class="chapter" data-level="3.18" data-path="regresión-lineal.html"><a href="regresión-lineal.html#valores-atípicos"><i class="fa fa-check"></i><b>3.18</b> Valores atípicos</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Aprendizaje Automático 1</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regresión-lineal" class="section level1" number="3">
<h1><span class="header-section-number">Chapter 3</span> Regresión lineal</h1>
<p>Este capítulo presenta la regresión lineal, el método de regresión paramétrica que usamos cuando la variable de resultado o respuesta es continua. Cuando el resultado es binario, utilizamos la regresión logística, tema de un capítulo posterior.</p>
<p>Qué pretendemos aprender en este capítulo:</p>
<ul>
<li>Entender qué pretende y cuándo se usa la regresión lineal</li>
<li>Cómo estimar los parámetros de un modelo de regresión</li>
<li>Familiarizarnos con algunas medidas usadas en la regresión lineal para valorar la utilidad del modelo</li>
<li>Tener una idea de otros aspectos a tener encuenta para estos modelos (suposiciones del modelo, colinealidad, valores atípicos, …)</li>
</ul>
<p>Existen numerosos recursos en la red para complementar este curso. Aquí tenéis algunos tutoriales/cursos en Datacamp:</p>
<p>– [DataCamp: Correlación y regresión] (<a href="https://www.datacamp.com/courses/correlation-and-regression" class="uri">https://www.datacamp.com/courses/correlation-and-regression</a>)</p>
<p>– [DataCamp: Intro to Statistics with R: Correlation and Linear Regression] (<a href="https://www.datacamp.com/courses/intro-to-statistics-with-r-correlation-and-linear-regression" class="uri">https://www.datacamp.com/courses/intro-to-statistics-with-r-correlation-and-linear-regression</a>)</p>
<p>– [Intro to Statistics with R: Multiple Regression] (<a href="https://www.datacamp.com/courses/intro-to-statistics-with-r-multiple-regression" class="uri">https://www.datacamp.com/courses/intro-to-statistics-with-r-multiple-regression</a>)</p>
<div id="preliminares" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Preliminares</h2>
<p>¿Qué son los modelos? Los modelos simplifican la realidad con fines de comprensión o predicción. Si bien pueden ser herramientas poderosas, debemos tener en cuenta que, después de todo, no son la realidad. En consecuencia, como se dice que dijo el estadístico George Box, “Todos los modelos son incorrectos, pero algunos son útiles”.</p>
<p>En términos generales, el modelado estadístico tiene estos dos objetivos a veces divergentes:</p>
<ol style="list-style-type: decimal">
<li><p><em>Descripción</em>: usar un modelo para describir la relación entre una variable de resultado de interés y una o más variables predictoras.</p></li>
<li><p><em>Predicción</em>: uso de un modelo para predecir instancias desconocidas de la variable de resultado de manera que se minimice el error predictivo fuera de la muestra.</p></li>
</ol>
<p>En el modelado, es posible centrarse en la descripción e ignorar la predicción, y viceversa. Por ejemplo, muchos algoritmos de aprendizaje automático son cajas negras: crean modelos que hacen un buen trabajo de predicción, pero son difíciles, si no imposibles, de interpretar y, en consecuencia, a menudo no nos ayudan a comprender las relaciones entre variables. La regresión lineal puede no ser la técnica más sofisticada, pero si se usa correctamente, su precisión predictiva compara bien con otros algoritmos más avanzados que veremos en este curso. Además, ofrece información descriptiva, en forma de coeficientes para cada variable, que son de gran utilida. La regresión lineal hace un buen trabajo con <em>tanto</em> descripción como predicción. En este capítulo aprenderemos estos usos de la regresión lineal.</p>
</div>
<div id="conceptos-básicos" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Conceptos básicos</h2>
<p>Comencemos por presentar brevemente el modelo lineal junto con algunos de los conceptos y terminología que usaremos a lo largo del curso.</p>
<p>Un modelo lineal es <em>paramétrico</em> porque asumimos que la relación entre dos variables es lineal y puede ser definida por los <em>parámetros</em> de una recta (el <em>intercept</em> y la pendiente). Comenzaremos considerando un modelo lineal simple.</p>
<div id="modelo-lineal-simple" class="section level3" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Modelo lineal simple</h3>
<p>Un modelo lineal simple tiene un resultado (outcome, variable predictiva), <span class="math inline">\(y\)</span>, y un predictor, <span class="math inline">\(x\)</span>. Está definido por la siguiente ecuación.</p>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1x_i + \epsilon_i,
\]</span></p>
<p>donde <span class="math inline">\(i = 1, \ldots, n.\)</span></p>
<p>El subíndice en esta ecuación, <span class="math inline">\(i\)</span>, indexa las observaciones <span class="math inline">\(n\)</span> en el conjunto de datos. (Pensemos en <span class="math inline">\(i\)</span> como un número de fila que corresponde a los datos de un individuo). La ecuación se puede leer de la siguiente manera: el valor de la <span class="math inline">\(i\)</span>-ésima variable resultado, <span class="math inline">\(y_i\)</span>, está definido por una <em>intercept</em>, <span class="math inline">\(\beta_0\)</span>, más una pendiente, <span class="math inline">\(\beta_1\)</span>, multiplicada por la variable predictora <span class="math inline">\(i\)</span>-ésima, <span class="math inline">\(x_i\)</span>. Estos elementos definen la parte <em>sistemática</em> o <em>determinista</em> del modelo. Sin embargo, debido a que el mundo es incierto y contiene aleatoriedad, sabemos que el modelo será incorrecto (estará sujeto a error). Para describir completamente los datos, necesitamos un término de error, <span class="math inline">\(\epsilon_i\)</span>, que también está indexado por fila. El término de error es la parte <em>estocástica</em> o <em>aleatoria</em> del modelo. <span class="math inline">\(\epsilon_i\)</span> mide la distancia entre los valores ajustados o esperados del modelo — calculados a partir de la parte determinista del modelo — y los valores reales. Los errores en un modelo lineal, también conocidos como residuales del modelo, son la parte de los datos que permanece sin explicar por la parte determinista del modelo. Uno de los supuestos clave de un modelo lineal es que los residuos se distribuyen normalmente con media = 0 y varianza = <span class="math inline">\(\sigma^2\)</span>, que denotamos, en notación matricial, como <span class="math inline">\(N (0, \sigma ^ 2)\)</span>.</p>
</div>
<div id="regresión-lineal-multivariante" class="section level3" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Regresión lineal multivariante</h3>
<p>Podemos agregar predictores adicionales, <span class="math inline">\(p\)</span>, a un modelo lineal simple, convirtiéndolo en un modelo lineal multivariante, que definimos de la siguiente manera:</p>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1 x_ {i1} + \cdots + \beta_p x_ {ip} + \varepsilon_i,
\]</span></p>
<p>donde <span class="math inline">\(i = 1, \ldots, n\)</span> y <span class="math inline">\(p = 1, \ldots, p.\)</span> En esta ecuación <span class="math inline">\(y_i\)</span> es nuevamente la variable resultado <span class="math inline">\(i\)</span>-ésima, <span class="math inline">\(\beta_0\)</span> es la <em>intercept</em>, <span class="math inline">\(\beta_1\)</span> es el coeficiente de la primera variable predictora, <span class="math inline">\(x_{1}\)</span>, <span class="math inline">\(\beta_p\)</span> es el coeficiente de la variable predictora <span class="math inline">\(p\)</span>-ésima, <span class="math inline">\(x_{p}\)</span>, y <span class="math inline">\(\epsilon_i\)</span> representa la parte estocástica del modelo, los residuos, indexados por fila. La parte determinista del modelo se puede resumir como <span class="math inline">\(X \beta\)</span>, una matriz <span class="math inline">\(p\)</span> x <span class="math inline">\(n\)</span>, que llamaremos el “predictor lineal”.</p>
</div>
<div id="incertidumbre" class="section level3" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Incertidumbre</h3>
<p>La incertidumbre es intrínseca al modelado estadístico. Distinguimos entre <em>Incertidumbre de estimación</em> e <em>Incertidumbre fundamental </em>:</p>
<ul>
<li><p>La incertidumbre de la estimación se deriva del desconocimiento de los parámetros <span class="math inline">\(\beta\)</span>. Disminuye a medida que <span class="math inline">\(n\)</span> aumenta y los <span class="math inline">\(SE\)</span>s se reducen.</p></li>
<li><p>La incertidumbre fundamental se deriva del componente estocástico del modelo, <span class="math inline">\(\epsilon\)</span>. Existe sin importar lo que haga el investigador, sin importar como de grande sea el tamaño muestral <span class="math inline">\(n\)</span>. Podemos reducir la incertidumbre fundamental con predictores elegidos inteligentemente, pero nunca podremos eliminarla.</p></li>
</ul>
</div>
</div>
<div id="ajuste-de-un-modelo-lineal" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Ajuste de un modelo lineal</h2>
<p>Para ajustar un modelo lineal usamos la función <code>lm()</code>. (La función <code>glm()</code> también se ajusta a un modelo lineal por defecto, definido por <code>family = gaussian</code>. También usaremos<code>glm()</code> para ajustar una regresión logística, con<code>family = binomial</code>). Por ejemplo, usemos el conjunto de dataos mtcars para averiguar si el consumo de combustible (mpg) está correlacionado con el peso del coche (wt). En R deberíamos ejecutar:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="regresión-lineal.html#cb65-1"></a><span class="kw">data</span>(mtcars)</span>
<span id="cb65-2"><a href="regresión-lineal.html#cb65-2"></a>(simple_model &lt;-<span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span>wt, <span class="dt">data =</span> mtcars))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt, data = mtcars)
## 
## Coefficients:
## (Intercept)           wt  
##      37.285       -5.344</code></pre>
<p>La ecuación del modelo es: <span class="math inline">\(\widehat {mpg} = 37.285 - 5.344wt\)</span>. (La notación del sombrero, <span class="math inline">\(\widehat {mpg}\)</span>, significa “estimación de”). Sin embargo, con el término de error incluido, ya no estamos <em>estimando</em> mpg sino describiéndolo exactamente: <span class="math inline">\(mpg = 37.285 - 5.344wt + error\)</span> (Por lo tanto, no hay notación de sombrero). Los componentes del modelo se pueden extraer del objeto del modelo usando <code>adjust()</code>, o, de manera equivalente en este caso, <code>predict()</code> y <code>residuals()</code>:</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="regresión-lineal.html#cb67-1"></a>mtcars_new &lt;-<span class="st"> </span>mtcars <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb67-2"><a href="regresión-lineal.html#cb67-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">cars =</span> <span class="kw">rownames</span>(mtcars),</span>
<span id="cb67-3"><a href="regresión-lineal.html#cb67-3"></a>         <span class="dt">fitted =</span> <span class="kw">fitted</span>(simple_model),</span>
<span id="cb67-4"><a href="regresión-lineal.html#cb67-4"></a>         <span class="dt">residuals =</span> <span class="kw">residuals</span>(simple_model)) <span class="op">%&gt;%</span></span>
<span id="cb67-5"><a href="regresión-lineal.html#cb67-5"></a><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">select</span>(cars, mpg, wt, fitted, residuals)</span>
<span id="cb67-6"><a href="regresión-lineal.html#cb67-6"></a></span>
<span id="cb67-7"><a href="regresión-lineal.html#cb67-7"></a><span class="kw">head</span>(mtcars_new)</span></code></pre></div>
<pre><code>##                cars  mpg    wt   fitted  residuals
## 1         Mazda RX4 21.0 2.620 23.28261 -2.2826106
## 2     Mazda RX4 Wag 21.0 2.875 21.91977 -0.9197704
## 3        Datsun 710 22.8 2.320 24.88595 -2.0859521
## 4    Hornet 4 Drive 21.4 3.215 20.10265  1.2973499
## 5 Hornet Sportabout 18.7 3.440 18.90014 -0.2001440
## 6           Valiant 18.1 3.460 18.79325 -0.6932545</code></pre>
<p>El modelo se puede utilizar para calcular valores ajustados para coches individuales en el conjunto de datos. Por ejemplo, el valor ajustado para el Mazda RX4, <span class="math inline">\(\widehat {mpg_1}\)</span>, se puede derivar de la ecuación del modelo, <span class="math inline">\(\beta_0 + \beta_1 x_ {i1}\)</span>: 37.29 - 5.34 x 2.62 = 23.28. (El valor <em>real</em> del Mazda RX4, calculado a partir del modelo, sería: 37.29 - 5.34 x 2.62 + 2.28 = 21). El modelo también se puede utilizar para la predicción. ¿Cuál sería el mpg para un coche que pesa 5000 libras? Según el modelo: 37,29 - 5,34 x 5 = 10.56.</p>
<div id="residuos" class="section level3" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Residuos</h3>
<p>Los residuales del modelo — representados por los segmentos de línea vertical en el gráfico siguiente — son las diferencias entre los valores ajustados y reales de mpg.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="regresión-lineal.html#cb69-1"></a><span class="kw">ggplot</span>(mtcars_new, <span class="kw">aes</span>(<span class="dt">x =</span> wt, <span class="dt">y =</span> mpg)) <span class="op">+</span></span>
<span id="cb69-2"><a href="regresión-lineal.html#cb69-2"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>, <span class="dt">color =</span> <span class="st">&quot;lightgrey&quot;</span>) <span class="op">+</span><span class="st">   </span></span>
<span id="cb69-3"><a href="regresión-lineal.html#cb69-3"></a><span class="st">  </span><span class="kw">geom_segment</span>(<span class="kw">aes</span>(<span class="dt">xend =</span> wt, <span class="dt">yend =</span> fitted), <span class="dt">alpha =</span> <span class="fl">.2</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb69-4"><a href="regresión-lineal.html#cb69-4"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb69-5"><a href="regresión-lineal.html#cb69-5"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y =</span> fitted), <span class="dt">shape =</span> <span class="dv">1</span>) <span class="op">+</span></span>
<span id="cb69-6"><a href="regresión-lineal.html#cb69-6"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Residuales del modelo para mpg ~ wt&quot;</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Podemos resumir los residuos con una medida llamada suma de cuadrados residual (RSS), que se calcula restando los resultados reales, <span class="math inline">\(y_i\)</span>, de los valores ajustados, <span class="math inline">\(\hat {y} _i\)</span>, elevando al cuadrado esas diferencias, luego sumando los cuadrados.</p>
<p><span class="math display">\[
\operatorname {RSS} = \sum_ {i = 1} ^ n ((\beta_0 + \beta_1x_i) - y_i) ^ 2 = \sum_{i = 1}^n(\hat {y} _i - y_i) ^ 2
\]</span></p>
<p>Al resumir los errores del modelo, RSS nos permite cuantificar el rendimiento del modelo con un solo número:</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="regresión-lineal.html#cb70-1"></a>rss &lt;-<span class="st"> </span><span class="cf">function</span>(fitted, actual){</span>
<span id="cb70-2"><a href="regresión-lineal.html#cb70-2"></a>  <span class="kw">sum</span>((fitted <span class="op">-</span><span class="st"> </span>actual)<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb70-3"><a href="regresión-lineal.html#cb70-3"></a>}</span>
<span id="cb70-4"><a href="regresión-lineal.html#cb70-4"></a></span>
<span id="cb70-5"><a href="regresión-lineal.html#cb70-5"></a><span class="kw">rss</span>(<span class="kw">fitted</span>(simple_model), mtcars<span class="op">$</span>mpg)</span></code></pre></div>
<pre><code>## [1] 278.3219</code></pre>
</div>
<div id="coeficientes-de-interpretación" class="section level3" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Coeficientes de interpretación</h3>
<p>¿Cómo interpretamos la salida de la función <code>lm()</code>? Comencemos con el modelo simple de mpg.</p>
<ul>
<li><strong>intercept</strong>: 37.29 representa el valor predicho de mpg cuando wt es 0. Dado que wt no puede ser igual a 0. El <em>intercept</em> no es interpretable en este modelo. Para hacerlo interpretable, necesitamos centrar la variable wt en 0, lo que podemos hacer fácilmente restando la media de wt de cada observación (<span class="math inline">\(x_ {centrado} = x - \ bar {x}\)</span>). Esta es una transformación lineal que cambiará la escala del predictor y, por lo tanto, <span class="math inline">\(\beta_0\)</span> también, pero no el ajuste del modelo: <span class="math inline">\(\beta_1\)</span> permanecerá igual (-5,34) al igual que RSS (278,32). Después de la transformación, el peso promedio del coche es 0 y el <em>intercept</em> representa las millas por galón pronosticadas para coches de peso promedio.</li>
</ul>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="regresión-lineal.html#cb72-1"></a>mtcars<span class="op">$</span>wt_centered &lt;-<span class="st"> </span>mtcars<span class="op">$</span>wt <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(mtcars<span class="op">$</span>wt)</span>
<span id="cb72-2"><a href="regresión-lineal.html#cb72-2"></a>(simple_model &lt;-<span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span>wt_centered, <span class="dt">data =</span> mtcars))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt_centered, data = mtcars)
## 
## Coefficients:
## (Intercept)  wt_centered  
##      20.091       -5.344</code></pre>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="regresión-lineal.html#cb74-1"></a><span class="kw">rss</span>(<span class="kw">fitted</span>(simple_model), mtcars<span class="op">$</span>mpg)</span></code></pre></div>
<pre><code>## [1] 278.3219</code></pre>
<p>Ahora el <em>intercept</em>, 20.09, es significativa y representa el valor predicho de mpg cuando wt_centered es 0 — es decir, cuando wt es promedio.</p>
<p>Hay dos formas de interpretar los coeficientes en un modelo lineal:</p>
<ol style="list-style-type: decimal">
<li><p><em>Contrafactual</em>: el coeficiente representa el cambio predicho en el resultado asociado con un aumento de 1 unidad en el predictor, mientras se mantienen constantes los demás predictores (en el caso multivariable).</p></li>
<li><p><em>Predictivo</em>: el coeficiente representa la diferencia pronosticada en el resultado entre dos grupos que difieren en 1 unidad en el predictor, mientras se mantienen constantes los otros predictores.</p></li>
</ol>
<p>Normalmente los coeficientes del modelo se suelen interpretar de acuerdo con el paradigma contrafáctico. Por lo tanto,</p>
<ul>
<li><em>wt_centered</em>: -5.34 representa el cambio previsto en el resultado, mpg, asociado con un aumento de 1 unidad en wt_centered.</li>
</ul>
<p>Agreguemos un segundo predictor al modelo, una versión binaria de caballos de fuerza (hp_bin), que definiremos como 0 para valores de hp que están por debajo del promedio y 1 para valores mayores o iguales que el promedio.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="regresión-lineal.html#cb76-1"></a>mtcars<span class="op">$</span>hp_bin &lt;-<span class="st"> </span><span class="kw">ifelse</span>(mtcars<span class="op">$</span>hp <span class="op">&lt;</span><span class="st"> </span><span class="kw">mean</span>(mtcars<span class="op">$</span>hp), <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb76-2"><a href="regresión-lineal.html#cb76-2"></a></span>
<span id="cb76-3"><a href="regresión-lineal.html#cb76-3"></a>(multivariable_model &lt;-<span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span>wt_centered <span class="op">+</span><span class="st"> </span>hp_bin , <span class="dt">data =</span> mtcars))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt_centered + hp_bin, data = mtcars)
## 
## Coefficients:
## (Intercept)  wt_centered       hp_bin  
##      21.649       -4.168       -3.324</code></pre>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="regresión-lineal.html#cb78-1"></a><span class="kw">rss</span>(<span class="kw">fitted</span>(multivariable_model), mtcars<span class="op">$</span>mpg)</span></code></pre></div>
<pre><code>## [1] 231.3121</code></pre>
<p>Este modelo multivariante es una mejora con respecto al modelo simple ya que tiene un RSS menor.</p>
<ul>
<li><p><strong>intercept</strong>: 21,65 representa el mpg predicho cuando los predictores continuos o binarios son iguales a 0 o (no aplicable en este caso) cuando las variables de los factores están en su nivel de referencia. El <em>intercept</em> es el mpg pronosticado por el modelo para autos de peso promedio que tienen caballos de fuerza por debajo del promedio.</p></li>
<li><p><em>wt_centered</em>: -4,17 representa el cambio previsto en mpg asociado con un aumento de 1 unidad en wt_centered (digamos, de 1 a 2) mientras se mantiene constante el otro predictor, hp_bin. <em>Los coeficientes de regresión multivariable capturan cómo el resultado varía de manera única con un predictor dado, después de tener en cuenta los efectos de todos los demás predictores.</em> En la práctica, esto significa que el coeficiente que describe la relación entre mpg y wt_centrado se ha promediado en los niveles hp_bin, por lo que es igual en cada nivel de hp_bin.</p></li>
<li><p><em>hp_bin</em>: -3.32 representa el cambio previsto en mpg asociado con un aumento de 1 unidad en hp_bin (de 0 a 1) mientras se mantiene constante el otro predictor, wt_centered.</p></li>
</ul>
</div>
<div id="interacciones" class="section level3" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Interacciones</h3>
<p>Podemos agregar una interacción a este modelo. A menudo, la relación entre un predictor y un resultado puede depender del nivel de otra variable predictiva. Por ejemplo, la pendiente de la recta de regresión que define la relación entre wt_centrado y mpg puede variar con los niveles de hp_bin. Si es así, decimos que existe una interacción entre wt_centered y hp_bin al predecir mpg. Para incluir una interacción entre dos variables en la fórmula del modelo, simplemente reemplazamos “+” por “*” en la fórmula del modelo. Esta fórmula, <code>mpg ~ wt_centered * hp_bin</code>, es exactamente equivalente a <code>mpg ~ wt_centered + wt_centered * hp_bin</code>, o a <code>mpg ~ wt_centered + hp_bin + wt_centered*hp_bin</code> ya que <code>lm ()</code> agrega automáticamente el efecto principal junto con la interacción. También se puede usar “:” para el término exacto de la interacción `<code>mpg ~ wt_centered + hp_bin + wt_centered:hp_bin</code>. Por “efecto principal” nos referimos a los términos que interactúan entre si. En este modelo, el efecto de interacción es wt_centered:hp_bin, mientras que wt_centered y hp_bin por sí mismos son los efectos principales.</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="regresión-lineal.html#cb80-1"></a>(multivariable_model &lt;-<span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span>wt_centered  <span class="op">*</span><span class="st"> </span>hp_bin, <span class="dt">data =</span> mtcars))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt_centered * hp_bin, data = mtcars)
## 
## Coefficients:
##        (Intercept)         wt_centered              hp_bin  wt_centered:hp_bin  
##             20.276              -6.391              -3.163               3.953</code></pre>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="regresión-lineal.html#cb82-1"></a><span class="kw">rss</span>(<span class="kw">fitted</span>(multivariable_model), mtcars<span class="op">$</span>mpg)</span></code></pre></div>
<pre><code>## [1] 170.3792</code></pre>
<p>RSS mejora una vez más.</p>
<p>Las interacciones pueden ser difíciles de interpretar es por ello que la visualización ayuda a comprender qué está sucediendo.</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="regresión-lineal.html#cb84-1"></a><span class="kw">ggplot</span>(mtcars, <span class="kw">aes</span>(wt_centered, mpg, <span class="dt">col =</span> <span class="kw">factor</span>(hp_bin), <span class="dt">group =</span> <span class="kw">factor</span>(hp_bin))) <span class="op">+</span></span>
<span id="cb84-2"><a href="regresión-lineal.html#cb84-2"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb84-3"><a href="regresión-lineal.html#cb84-3"></a><span class="st">  </span><span class="kw">stat_smooth</span>(<span class="dt">method=</span><span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> F) <span class="op">+</span></span>
<span id="cb84-4"><a href="regresión-lineal.html#cb84-4"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;mpg ~ wt_centered * hp_bin&quot;</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Podemos ver que la relación entre wt y mpg depende de los niveles de hp_bin: las pendientes de las rectas de regresión difieren. <em>Las rectas de regresión no paralelas indican la presencia de una interacción</em>. Observamos una relación más fuerte entre el peso y las millas por galón entre los autos con caballos de fuerza por debajo del promedio (una relación <em>negativa</em> más fuerte) que entre los autos con más caballos de fuerza. Las rectas de regresión para hp_bin se vuelven más planas a medida que aumenta wt_centrado.</p>
<p><em>NOTA: la presencia de una interacción cambia la interpretación de los efectos principales</em>. En un modelo sin interacción, los efectos principales son independientes de los valores particulares de los otros predictores. Por el contrario, una interacción hace que los efectos principales dependan de valores particulares de los otros predictores.</p>
<ul>
<li><p><em>wt_centered:hp_bin</em>: 3.95 representa la diferencia en la pendiente de wt_centered para hp_bin = 1 en comparación con hp_bin = 0. En otras palabras, cuando aumentamos hp_bin de 0 a 1, se predice que la pendiente de la rectas de regresión para wt_centered aumentar en 3,95. O, cuando aumentamos wt_centrado en 1, se predice que la rectas de regresión para hp_bin aumentará en 3,95.</p></li>
<li><p><em>wt_centered</em>: -6.39 representa el cambio predicho en mpg asociado con un aumento de 1 unidad en wt <em>entre aquellos coches donde hp_bin = 0.</em></p></li>
<li><p><em>hp_bin</em>: -3.16 representa el cambio previsto en mpg asociado con un aumento de 1 unidad en hp_bin de 0 a 1 <em>entre los coches con wt_centered = 0</em> (promedio).</p></li>
</ul>
<p>Puede resultar instructivo ver qué está haciendo <code>lm ()</code> en segundo plano para ajustarse a este modelo. El comando <code>model.matrix ()</code> muestra cómo se ha reformateado la matriz del predictor para la regresión:</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="regresión-lineal.html#cb85-1"></a><span class="kw">head</span>(<span class="kw">model.matrix</span>(multivariable_model))</span></code></pre></div>
<pre><code>##                   (Intercept) wt_centered hp_bin wt_centered:hp_bin
## Mazda RX4                   1    -0.59725      0            0.00000
## Mazda RX4 Wag               1    -0.34225      0            0.00000
## Datsun 710                  1    -0.89725      0            0.00000
## Hornet 4 Drive              1    -0.00225      0            0.00000
## Hornet Sportabout           1     0.22275      1            0.22275
## Valiant                     1     0.24275      0            0.00000</code></pre>
<p>El <em>intercept</em> es un vector de 1s. El vector para el término de interacción, wt_centered: hp_bin, consiste simplemente en el producto de los dos vectores con las componentes de cada variable</p>
</div>
</div>
<div id="estimación-por-mínimos-cuadrados" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Estimación por mínimos cuadrados</h2>
<p>Para el modelo <span class="math inline">\(y = X \beta + \epsilon\)</span>, donde <span class="math inline">\(\beta\)</span> es el vector de coeficientes ajustados y $$ es el vector de residuos del modelo, la estimación de mínimos cuadrados es <span class="math inline">\(\hat {\beta}\)</span> que minimiza RSS para los datos dados <span class="math inline">\(X, y\)</span>. Podemos expresar la estimación de mínimos cuadrados como <span class="math inline">\(\ hat {\beta} = (X&#39;X) ^ {- 1} X&#39;y\)</span>, donde <span class="math inline">\(X&#39;\)</span> es la transposición de la matriz de <span class="math inline">\(X\)</span>. A continuación podemos ver cómo se deriva esta fórmula<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p><span class="math display">\[
RSS = \epsilon ^ 2 = (y - X \beta) &#39;(y - X \beta)
\]</span></p>
<p><span class="math display">\[
RSS = y&#39;y - y&#39;X \beta - \beta&#39;X&#39;y + \beta&#39;X&#39;X \beta
\]</span></p>
<p><span class="math display">\[
RSS = y&#39;y - (2y&#39;X) \beta + \beta &#39;(X&#39;X) \beta
\]</span></p>
<p>Según apunta el autor: “Aunque la multiplicación de matrices generalmente no es conmutativa, cada producto [arriba] es 1 x 1, por lo que <span class="math inline">\(y&#39;X \beta = \beta&#39;X&#39;y\)</span>”.</p>
<p>Para minimizar RSS encontramos la derivada parcial con respecto a <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[
\frac{\partial RSS}{\partial\beta}= 0  - 2X&#39;y + 2X&#39;X\beta
\]</span></p>
<p>Establecemos esta derivada igual a 0 y resolvemos $$:</p>
<p><span class="math display">\[
X&#39;X \beta = X&#39;y
\]</span></p>
<p><span class="math display">\[
\beta = (X&#39;X) ^ {- 1} X&#39;y
\]</span></p>
<p>Podemos usar esta ecuación y la matriz del modelo para el modelo multivariable para estimar <span class="math inline">\(\hat{\beta}\)</span> para <code>mpg ~ wt_centered * hp_bin + hp_centered</code>:</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="regresión-lineal.html#cb87-1"></a>X &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(multivariable_model)</span>
<span id="cb87-2"><a href="regresión-lineal.html#cb87-2"></a>y &lt;-<span class="st"> </span>mtcars<span class="op">$</span>mpg</span>
<span id="cb87-3"><a href="regresión-lineal.html#cb87-3"></a></span>
<span id="cb87-4"><a href="regresión-lineal.html#cb87-4"></a><span class="kw">solve</span>(<span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>X) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>y</span></code></pre></div>
<pre><code>##                         [,1]
## (Intercept)        20.276155
## wt_centered        -6.390834
## hp_bin             -3.162983
## wt_centered:hp_bin  3.953027</code></pre>
<p>Este método devuelve las mismas estimaciones de coeficientes que <code>lm ()</code>.</p>
</div>
<div id="medidas-adicionales-de-ajuste-del-modelo" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Medidas adicionales de ajuste del modelo</h2>
<p>Como hemos visto, RSS nos permite comparar qué tan bien se ajustan los modelos a los datos. Una medida relacionada es la raíz del error cuadrático medio (RMSE por sus siglas en inglés), la raíz cuadrada del promedio de los errores cuadráticos:</p>
<p><span class="math display">\[
\operatorname{RMSE}= \sqrt{\frac{\sum_{i=1}^n ((\beta_0 + \beta_1x_i) - y_i)^2}{n}} 
\]</span></p>
<p><span class="math display">\[
= \sqrt{\frac{\sum_{i=1}^n (\hat{y}_i - y_i)^2}{n}}
\]</span></p>
<p>Lo bueno de RMSE es que, a diferencia de RSS, devuelve un valor que está en la escala del variable resultado.</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="regresión-lineal.html#cb89-1"></a><span class="kw">rss</span>(<span class="kw">fitted</span>(multivariable_model), mtcars<span class="op">$</span>mpg)</span></code></pre></div>
<pre><code>## [1] 170.3792</code></pre>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="regresión-lineal.html#cb91-1"></a>rmse &lt;-<span class="st"> </span><span class="cf">function</span>(fitted, actual){</span>
<span id="cb91-2"><a href="regresión-lineal.html#cb91-2"></a>  <span class="kw">sqrt</span>(<span class="kw">mean</span>((fitted <span class="op">-</span><span class="st"> </span>actual)<span class="op">^</span><span class="dv">2</span>))</span>
<span id="cb91-3"><a href="regresión-lineal.html#cb91-3"></a>}</span>
<span id="cb91-4"><a href="regresión-lineal.html#cb91-4"></a></span>
<span id="cb91-5"><a href="regresión-lineal.html#cb91-5"></a><span class="kw">rmse</span>(<span class="kw">fitted</span>(multivariable_model), mtcars<span class="op">$</span>mpg)</span></code></pre></div>
<pre><code>## [1] 2.307455</code></pre>
<p>En promedio, entonces, este modelo tiene una diferencia de aproximadamente 2.31 mpg por cada coche.</p>
<p><span class="math inline">\(R^2\)</span> es otra medida de ajuste del modelo que es conveniente porque es una medida estandarizada — escalada entre 0 y 1 — y, por lo tanto, es comparable en todos los contextos.</p>
<p><span class="math display">\[
R^2 = 1 - \frac{SS_\text{resid}}{SS_\text{tot}}, 
\]</span></p>
<p>donde <span class="math inline">\(SS_\text{tot} = \sum_i (y_i- \bar{y}) ^ 2\)</span> y <span class="math inline">\(SS_\text{res} = \sum_i (y_i - \hat {y} _i) ^ 2\)</span>. En resumen: <span class="math inline">\(R^2\)</span> representa la variación en la variable de resultado explicada por el modelo como una proporción de la variación total. En la gráfica de abajo, el panel de la izquierda, TSS, sirve como denominador para calcular <span class="math inline">\(R^2\)</span>, y el panel de la derecha, RSS, es el numerador.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="regresión-lineal.html#cb93-1"></a><span class="kw">require</span>(gridExtra)</span>
<span id="cb93-2"><a href="regresión-lineal.html#cb93-2"></a></span>
<span id="cb93-3"><a href="regresión-lineal.html#cb93-3"></a>mtcars<span class="op">$</span>mean &lt;-<span class="st"> </span><span class="kw">mean</span>(mtcars<span class="op">$</span>mpg) </span>
<span id="cb93-4"><a href="regresión-lineal.html#cb93-4"></a>plot1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(mtcars, <span class="kw">aes</span>(wt, mpg)) <span class="op">+</span></span>
<span id="cb93-5"><a href="regresión-lineal.html#cb93-5"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept=</span><span class="kw">mean</span>(mtcars<span class="op">$</span>mpg), <span class="dt">col =</span> <span class="dv">2</span>) <span class="op">+</span><span class="st">  </span></span>
<span id="cb93-6"><a href="regresión-lineal.html#cb93-6"></a><span class="st">  </span><span class="kw">geom_segment</span>(<span class="kw">aes</span>(<span class="dt">xend =</span> wt, <span class="dt">yend =</span> mean), <span class="dt">alpha =</span> <span class="fl">.2</span>) <span class="op">+</span><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb93-7"><a href="regresión-lineal.html#cb93-7"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="kw">paste</span>(<span class="st">&quot;Total Sum of Squares (TSS) </span><span class="ch">\n</span><span class="st">&quot;</span>,tss))</span>
<span id="cb93-8"><a href="regresión-lineal.html#cb93-8"></a></span>
<span id="cb93-9"><a href="regresión-lineal.html#cb93-9"></a>plot2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(mtcars, <span class="kw">aes</span>(wt, mpg)) <span class="op">+</span></span>
<span id="cb93-10"><a href="regresión-lineal.html#cb93-10"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>, <span class="dt">col=</span><span class="dv">2</span>) <span class="op">+</span><span class="st">  </span></span>
<span id="cb93-11"><a href="regresión-lineal.html#cb93-11"></a><span class="st">  </span><span class="kw">geom_segment</span>(<span class="kw">aes</span>(<span class="dt">xend =</span> wt, <span class="dt">yend =</span> <span class="kw">fitted</span>(<span class="kw">lm</span>(mpg<span class="op">~</span>wt, <span class="dt">data=</span>mtcars))), <span class="dt">alpha =</span> <span class="fl">.2</span>) <span class="op">+</span><span class="st">  </span></span>
<span id="cb93-12"><a href="regresión-lineal.html#cb93-12"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb93-13"><a href="regresión-lineal.html#cb93-13"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="kw">paste</span>(<span class="st">&quot;Residual Sum of Squares (RSS) </span><span class="ch">\n</span><span class="st">&quot;</span>,rss))</span>
<span id="cb93-14"><a href="regresión-lineal.html#cb93-14"></a></span>
<span id="cb93-15"><a href="regresión-lineal.html#cb93-15"></a><span class="kw">grid.arrange</span>(plot1, plot2, <span class="dt">ncol=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Para nuestro modelo lineal simple, mpg ~ wt, <span class="math inline">\(R^2\)</span> era de .75, que coincide con nuestro cálculo aquí: 1 - 278/1126 = .75. Esto significa que wt explica el 75% de la variación total en mpg. Cuanto mejor se ajusta la regresión lineal a los datos en comparación con el promedio simple, más se acerca el valor de <span class="math inline">\(R^2\)</span> a 1. Para la regresión lineal simple, <span class="math inline">\(R^2\)</span> es como la correlación al cuadrado entre el resultado y el predictor.</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="regresión-lineal.html#cb94-1"></a><span class="kw">cor</span>(mtcars<span class="op">$</span>mpg, mtcars<span class="op">$</span>wt)<span class="op">^</span><span class="dv">2</span></span></code></pre></div>
<pre><code>## [1] 0.7528328</code></pre>
<p>Un problema con el <span class="math inline">\(R^2\)</span> es que agregar variables al modelo tiende a mejorarlo aunque las nuevas variables no sean relevantes. Añadir más variables puede conducir a un sobreajuste. Se ha desarrollado una variante de <span class="math inline">\(R^2\)</span> que penaliza la medida para predictores adicionales: <span class="math inline">\(R^2\)</span>ajustados.</p>
<p><span class="math display">\[
\bar R^2 = {1-(1-R^2){n-1 \over n-p-1}} 
\bar R^2 = {R^2-(1-R^2){p \over n-p-1}},
\]</span></p>
<p>donde <span class="math inline">\(n\)</span> es el número de observaciones en el conjunto de datos y <span class="math inline">\(p\)</span> es el número de predictores en el modelo.</p>
</div>
<div id="sesgo-variación-sobreajuste" class="section level2" number="3.6">
<h2><span class="header-section-number">3.6</span> Sesgo, variación, sobreajuste</h2>
<p>¿Qué entendemos por “sobreajuste”? Los siguientes son conceptos clave para pensar en el rendimiento del modelo, a los que volveremos a lo largo del curso:</p>
<ul>
<li><p><em>Rendimiento en la muestra</em>: cómo se comporta el modelo en los datos que se utilizaron para construirlo.</p></li>
<li><p><em>Rendimiento fuera de la muestra</em>: cómo se comporta el modelo cuando encuentra nuevos datos.</p></li>
<li><p>Si el modelo funciona mejor dentro de la muestra que fuera de la muestra, entonces decimos que el modelo <em>sobreajusta</em> los datos de la muestra o de entrenamiento.</p></li>
<li><p>El sobreajuste ocurre cuando un modelo se ajusta a la muestra <em>demasiado bien</em>: el modelo ha sido optimizado para capturar las idiosincrasias — el ruido aleatorio — de la muestra.</p></li>
<li><p><em>Sesgo</em> se refiere a una alta precisión predictiva en la muestra. El sesgo bajo es bueno.</p></li>
<li><p><em>Varianza</em> se refiere a una mayor precisión predictiva dentro de la muestra que fuera de la muestra. La varianza baja es buena.</p></li>
<li><p>Un modelo que sobreajusta tiene un sesgo bajo y una gran varianza.</p></li>
<li><p><em>La compensación de sesgo-varianza</em> se refiere a la idea de que no se puede tener un sesgo bajo y una varianza baja a la vez.</p></li>
</ul>
<p>Nos protegeremos contra — o evaluaremos la cantidad de — sobreajuste usando una técnica llamada validación cruzada que veremos más adelante.</p>
</div>
<div id="regresión-como-estimación-de-una-media-condicional" class="section level2" number="3.7">
<h2><span class="header-section-number">3.7</span> Regresión como estimación de una media condicional</h2>
<p>Dadas las complejidades anteriores, se podría pensar que el uso de la regresión lineal sólo es útil para nada más (¡y nada menos!) estimar una media condicional. Pero, ¿qué es una media condicional?</p>
<p>Consideremos el siguiente ejemplo. En 2011 se inició un programa de uso compartido de bicicletas en USA y se recopilaron datos durante 2011 y 2012 sobre el uso estacional de bicicletas y las condiciones climáticas. Estos datos se encuentran recogidos en el fichero <a href="https://raw.githubusercontent.com/isglobal-brge/TeachingMaterials/master/Aprendizaje_Automatico_1/data/day.csv">day.csv</a>. La variable de resultado que nos interesa es " contar "— el número total de ciclistas que alquilan bicicletas en un día determinado. El conjunto de datos tiene una fila para cada día, con variables para (entre otras) estación, año, mes, feriado, día de la semana, temperatura, temperatura percibida, humedad y velocidad del viento (nombradas en inglés).</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="regresión-lineal.html#cb96-1"></a>day &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;data/day.csv&quot;</span>)</span>
<span id="cb96-2"><a href="regresión-lineal.html#cb96-2"></a></span>
<span id="cb96-3"><a href="regresión-lineal.html#cb96-3"></a>day &lt;-<span class="st"> </span>day <span class="op">%&gt;%</span></span>
<span id="cb96-4"><a href="regresión-lineal.html#cb96-4"></a><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="dt">count =</span> cnt,</span>
<span id="cb96-5"><a href="regresión-lineal.html#cb96-5"></a>         season,</span>
<span id="cb96-6"><a href="regresión-lineal.html#cb96-6"></a>         <span class="dt">year=</span> yr,</span>
<span id="cb96-7"><a href="regresión-lineal.html#cb96-7"></a>         <span class="dt">month =</span> mnth,</span>
<span id="cb96-8"><a href="regresión-lineal.html#cb96-8"></a>         holiday,</span>
<span id="cb96-9"><a href="regresión-lineal.html#cb96-9"></a>         weekday,</span>
<span id="cb96-10"><a href="regresión-lineal.html#cb96-10"></a>         <span class="dt">temperature =</span> temp,</span>
<span id="cb96-11"><a href="regresión-lineal.html#cb96-11"></a>         atemp,</span>
<span id="cb96-12"><a href="regresión-lineal.html#cb96-12"></a>         <span class="dt">humidity =</span> hum,</span>
<span id="cb96-13"><a href="regresión-lineal.html#cb96-13"></a>         windspeed)</span></code></pre></div>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="regresión-lineal.html#cb97-1"></a><span class="kw">glimpse</span>(day)</span></code></pre></div>
<pre><code>## Rows: 731
## Columns: 10
## $ count       &lt;int&gt; 985, 801, 1349, 1562, 1600, 1606, 1510, 959, 822, 1321, 1263, 1162, 1406, 1421, 1248, 1204, 1000, 683, 1650, 192...
## $ season      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...
## $ year        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
## $ month       &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2...
## $ holiday     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
## $ weekday     &lt;int&gt; 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1...
## $ temperature &lt;dbl&gt; 0.3441670, 0.3634780, 0.1963640, 0.2000000, 0.2269570, 0.2043480, 0.1965220, 0.1650000, 0.1383330, 0.1508330, 0....
## $ atemp       &lt;dbl&gt; 0.3636250, 0.3537390, 0.1894050, 0.2121220, 0.2292700, 0.2332090, 0.2088390, 0.1622540, 0.1161750, 0.1508880, 0....
## $ humidity    &lt;dbl&gt; 0.805833, 0.696087, 0.437273, 0.590435, 0.436957, 0.518261, 0.498696, 0.535833, 0.434167, 0.482917, 0.686364, 0....
## $ windspeed   &lt;dbl&gt; 0.1604460, 0.2485390, 0.2483090, 0.1602960, 0.1869000, 0.0895652, 0.1687260, 0.2668040, 0.3619500, 0.2232670, 0....</code></pre>
<p>Una pregunta exploratoria inicial es: ¿cómo varía el uso de la bicicleta por año? Podemos responder a esta pregunta simplemente calculando un promedio para cada año:</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="regresión-lineal.html#cb99-1"></a>day <span class="op">%&gt;%</span></span>
<span id="cb99-2"><a href="regresión-lineal.html#cb99-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">year =</span> <span class="kw">ifelse</span>(year <span class="op">==</span><span class="st"> </span><span class="dv">0</span>, <span class="dv">2011</span>, <span class="dv">2012</span>)) <span class="op">%&gt;%</span></span>
<span id="cb99-3"><a href="regresión-lineal.html#cb99-3"></a><span class="st">  </span><span class="kw">group_by</span>(year) <span class="op">%&gt;%</span></span>
<span id="cb99-4"><a href="regresión-lineal.html#cb99-4"></a><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">summarize</span>(<span class="st">`</span><span class="dt">average ridership</span><span class="st">`</span> =<span class="st"> </span><span class="kw">round</span>(<span class="kw">mean</span>(count)))</span></code></pre></div>
<pre><code>## # A tibble: 2 x 2
##    year `average ridership`
##   &lt;dbl&gt;               &lt;dbl&gt;
## 1  2011                3406
## 2  2012                5600</code></pre>
<p>El número de pasajeros promedio en este resumen representa una media condicional: la media de la variable de recuento, condicional al año. ¿Qué nos dice la regresión lineal sobre el promedio de pasajeros (riders) por año?</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="regresión-lineal.html#cb101-1"></a><span class="kw">lm</span>(count <span class="op">~</span><span class="st"> </span>year, <span class="dt">data =</span> day)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = count ~ year, data = day)
## 
## Coefficients:
## (Intercept)         year  
##        3406         2194</code></pre>
<p>La salida del modelo incluye un <em>intercept</em> y un coeficiente para el año. El <em>intercept</em> es el promedio de la variable de resultado cuando los predictores numéricos son iguales a cero. Por lo tanto, 3406 es el número de pasajeros promedio cuando año = 0 (es decir, 2011), y el coeficiente para el año, 2194, es el aumento esperado en el número de pasajeros asociado con un aumento de 1 unidad en el año (es decir, cuando el año pasa de 0 a 1 ). Por lo tanto, el número promedio de pasajeros en el año 1 (2012) es solo la suma de los dos coeficientes — 3406 + 2194 = 5600 — que es idéntica a la media condicional para 2012 que calculamos anteriormente usando<code>dplyr</code> .</p>
<p>En general, dadas dos variables aleatorias, X e Y (piense: año y número de pasajeros), podemos definir la media condicional como el valor esperado o promedio de Y dado que X está restringido a tener un valor específico, <span class="math inline">\(x\)</span>, a partir de su rango: <span class="math inline">\(\mathbf {E} [Y \mid X = x]\)</span>. Ejemplo: <span class="math inline">\(\mathbf{E} [Ridership \mid Year = 2012]\)</span> es el número promedio de pasajeros dado ese año = 2012. Una media condicional tiene un valor descriptivo — sabemos que el coeficiente $$para el año de nuestro modelo, 2194, representa la relación entre el número de pasajeros y el año, con la magnitud o el valor absoluto del coeficiente que indica la fuerza de la relación, positiva o negativa. Los coeficientes también se pueden utilizar para la predicción. ¿Cuántos ciclistas más deberíamos esperar en 2013? Utilice el modelo: <span class="math inline">\(\mathbf {E} [riders \mid year = 2013]\)</span> es igual al número en 2012, 5600, más el coeficiente del año: 5600 + 2194 = 7794.</p>
<p>El modelo nos permite predecir, pero debemos recordar que no hay nada mágico en la predicción. Deberíamos pensar críticamente al respecto. Por un lado, asume una tendencia constante año tras año. ¿Es esta una suposición razonable?</p>
</div>
<div id="la-función-de-regresión" class="section level2" number="3.8">
<h2><span class="header-section-number">3.8</span> La función de regresión</h2>
<p>Consideremos la cantidad media de pasajeros condicionada a la temperatura (que en este conjunto de datos se ha normalizado y convertido a grados Celsius). Podemos definir una función que devolverá la media condicional. Para cualquier temperatura, <span class="math inline">\(t\)</span>, defina <span class="math inline">\(\mu (t) = \mathbf {E} [Riders \mid Temperature = t]\)</span>, que es el número medio de pasajeros cuando temperatura = <span class="math inline">\(t\)</span>. Dado que podemos variar <span class="math inline">\(t\)</span>, esto es de hecho una función, y se conoce como la <em>función de regresión</em> que relaciona a los pasajeros con la temperatura. Por ejemplo, <span class="math inline">\(\mu\)</span> (.68) es el número medio de pasajeros cuando <span class="math inline">\(t\)</span>= .68 y <span class="math inline">\(\mu\)</span> (.05)es el número medio de pasajeros cuando <span class="math inline">\(t\)</span>= .05, etc.</p>
<p>Debemos tener en cuenta que el valor real de <span class="math inline">\(\mu\)</span> (.68) se desconoce porque es un valor de población. Existe, pero no en nuestra muestra. Entonces, nuestra estimación, <span class="math inline">\(\hat {\ mu}\)</span> (. 68), debe basarse en los pares de Riders-Temperature que tenemos en nuestros datos: <span class="math inline">\((r_ {1}, t_ {1}), ..., ( r_ {731}, t_ {731})\)</span>. ¿Cómo podemos hacer esto exactamente? Un enfoque sería simplemente calcular las medias condicionales relevantes a partir de nuestros datos. Para encontrar <span class="math inline">\(\hat {\mu} (t)\)</span> usando este método, simplificaremos los datos redondeando la temperatura a dos lugares decimales.</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="regresión-lineal.html#cb103-1"></a>day <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb103-2"><a href="regresión-lineal.html#cb103-2"></a><span class="st">  </span><span class="kw">group_by</span>(<span class="dt">temperature =</span> <span class="kw">round</span>(temperature, <span class="dv">2</span>)) <span class="op">%&gt;%</span></span>
<span id="cb103-3"><a href="regresión-lineal.html#cb103-3"></a><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">summarize</span>(<span class="dt">mean =</span> <span class="kw">round</span>(<span class="kw">mean</span>(count))) </span></code></pre></div>
<pre><code>## # A tibble: 77 x 2
##    temperature  mean
##          &lt;dbl&gt; &lt;dbl&gt;
##  1        0.06   981
##  2        0.1   1201
##  3        0.11  2368
##  4        0.13  1567
##  5        0.14  1180
##  6        0.15  1778
##  7        0.16  1441
##  8        0.17  1509
##  9        0.18  1597
## 10        0.19  2049
## # ... with 67 more rows</code></pre>
<p>Sin embargo, se puede observar que faltan valores en la secuencia. Aquí hay un gráfico que muestra las brechas en los datos.</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="regresión-lineal.html#cb105-1"></a>day <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb105-2"><a href="regresión-lineal.html#cb105-2"></a><span class="st">  </span><span class="kw">group_by</span>(<span class="dt">temperature =</span> <span class="kw">round</span>(temperature, <span class="dv">2</span>)) <span class="op">%&gt;%</span></span>
<span id="cb105-3"><a href="regresión-lineal.html#cb105-3"></a><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">summarize</span>(<span class="dt">mean =</span> <span class="kw">round</span>(<span class="kw">mean</span>(count))) <span class="op">%&gt;%</span></span>
<span id="cb105-4"><a href="regresión-lineal.html#cb105-4"></a><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">right_join</span>(<span class="kw">data.frame</span>(<span class="dt">temperature=</span><span class="kw">seq</span>(.<span class="dv">01</span>, <span class="fl">.9</span>, <span class="fl">.01</span>)), <span class="dt">by =</span> <span class="st">&quot;temperature&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb105-5"><a href="regresión-lineal.html#cb105-5"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(temperature, mean)) <span class="op">+</span></span>
<span id="cb105-6"><a href="regresión-lineal.html#cb105-6"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb105-7"><a href="regresión-lineal.html#cb105-7"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Estimated mean daily riders (with missing observations)&quot;</span>) <span class="op">+</span></span>
<span id="cb105-8"><a href="regresión-lineal.html#cb105-8"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;temperature&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;mean riders&quot;</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/temp2-1.png" width="672" /></p>
<p>Este enfoque para estimar la función de regresión, <span class="math inline">\(\hat{\mu} (t)\)</span> tendrá problemas cuando, por ejemplo, queremos predecir el número de pasajeros a una temperatura para la que no tenemos datos.</p>
<p>Usar medias condicionales para estimar <span class="math inline">\(\hat {\mu} (t)\)</span> es un enfoque <em>no paramétrico</em>. Es decir, no asumimos nada sobre la forma de la función desconocida <span class="math inline">\(\mu(t)\)</span> si se trazara en un gráfico, sino que simplemente la estimamos directamente a partir de nuestros datos. La regresión de K-vecinos más cercanos (KNN) es una generalización de este enfoque no paramétrico. Este tipo de regresión es muy útil cuando queremos describir cuál es la relación entre nuestros datos sin asumir que dicha relación es lineal (que puede que sea el caso).</p>
<p>Debemos tener en cuenta que podríamos hacer algunas suposiciones sobre esa forma, posiblemente mejorando nuestras estimaciones, lo que haría que nuestro enfoque sea <em>paramétrico</em>, como en el caso de la regresión lineal.</p>
</div>
<div id="estimación-no-paramétrica-de-la-función-de-regresión-regresión-knn" class="section level2" number="3.9">
<h2><span class="header-section-number">3.9</span> Estimación no paramétrica de la función de regresión: regresión KNN</h2>
<p>A veces no es posible calcular buenas medias condicionales para el resultado debido a la falta de valores de predicción. Supongamos que queremos encontrar <span class="math inline">\(\hat{\mu} (. 12)\)</span>. Resulta que no hubo días en nuestro conjunto de datos en los que la temperatura fuera de .12. El algoritmo KNN resuelve este problema usando las <span class="math inline">\(k\)</span> observaciones más cercanas a <span class="math inline">\(t\)</span>= .12 para calcular la media condicional, <span class="math inline">\(\hat {\mu} (. 12)\)</span>. Si definimos <span class="math inline">\(k\)</span>= 4, entonces tomaríamos los <em>cuatro</em> valores más cercanos a .12 en el conjunto de datos — .1275, .134783, .138333, .1075. (“Más cercano” en este caso se define como la distancia euclidiana, que en un espacio unidimensional, una recta numérica, es simplemente: <span class="math inline">\(\sqrt {(xy) ^ 2}\)</span>.) Estos <span class="math inline">\(k\)</span>= 4 valores más cercanos se usarían para calcular <span class="math inline">\(\hat{\mu} (. 12)\)</span> calculando el promedio.</p>
<p>Establecer el valor de <span class="math inline">\(k\)</span> es obviamente una decisión crítica. Si usamos <span class="math inline">\(k\)</span>= 100, por ejemplo, nuestras estimaciones podrían no ser muy buenas. Y quizás <span class="math inline">\(k\)</span>= 4 sea demasiado bajo — podría llevar a un sobreajuste. En el caso de <span class="math inline">\(k\)</span>= 4, el error dentro de la muestra (sesgo) puede ser bajo, pero el error al predecir nuevas observaciones (varianza) puede ser alto.</p>
<p>Aquí, por ejemplo, hay una gráfica de los valores ajustados de un modelo KNN de la temperatura de los pasajeros cuando <span class="math inline">\(k\)</span>= 4.</p>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>El sesgo en este modelo es presumiblemente bajo porque el ajuste es muy flexible: los valores ajustados están muy cerca de los valores reales. El problema es que la función de regresión KNN en <span class="math inline">\(k\)</span>= 4 podría estar haciendo <em>demasiado</em> buen trabajo al describir la muestra. Cuando este modelo encuentra nuevos datos, sin las mismas idiosincrasias, su rendimiento puede ser muy pobre, con una gran variación. Es posible que el modelo esté sobre-ajustado a la muestra. La compensación de sesgo-varianza expresa esta idea: cuando el sesgo es bajo, es probable que la varianza sea alta, y viceversa.</p>
<p>A continuación se muestra un ejemplo de un sesgo más alto, posiblemente un caso de varianza menor cuando <span class="math inline">\(k\)</span>= 40.</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="regresión-lineal.html#cb106-1"></a>r &lt;-<span class="st"> </span>day</span>
<span id="cb106-2"><a href="regresión-lineal.html#cb106-2"></a>r<span class="op">$</span>temp_rounded &lt;-<span class="st"> </span><span class="kw">round</span>(r<span class="op">$</span>temp,<span class="dv">2</span>)</span>
<span id="cb106-3"><a href="regresión-lineal.html#cb106-3"></a><span class="kw">library</span>(dplyr)</span>
<span id="cb106-4"><a href="regresión-lineal.html#cb106-4"></a>knn_fit &lt;-<span class="st"> </span><span class="kw">knn.reg</span>(<span class="dt">train=</span>r<span class="op">$</span>temperature, <span class="dt">y=</span>r<span class="op">$</span>count, <span class="dt">k=</span><span class="dv">40</span>,</span>
<span id="cb106-5"><a href="regresión-lineal.html#cb106-5"></a>                   <span class="dt">algorithm=</span><span class="st">&quot;brute&quot;</span>)</span>
<span id="cb106-6"><a href="regresión-lineal.html#cb106-6"></a>r<span class="op">$</span>knn &lt;-<span class="st"> </span>knn<span class="op">$</span>pred</span>
<span id="cb106-7"><a href="regresión-lineal.html#cb106-7"></a> </span>
<span id="cb106-8"><a href="regresión-lineal.html#cb106-8"></a>est &lt;-<span class="st">  </span>r <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(temp_rounded) <span class="op">%&gt;%</span><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">summarize</span>(<span class="dt">count=</span><span class="kw">mean</span>(count), <span class="dt">knn=</span><span class="kw">mean</span>(knn))</span>
<span id="cb106-9"><a href="regresión-lineal.html#cb106-9"></a></span>
<span id="cb106-10"><a href="regresión-lineal.html#cb106-10"></a><span class="kw">plot</span>(<span class="dt">x=</span>est<span class="op">$</span>temp_rounded, <span class="dt">y=</span>est<span class="op">$</span>count, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">ylab=</span><span class="st">&quot;mean riders&quot;</span>,</span>
<span id="cb106-11"><a href="regresión-lineal.html#cb106-11"></a>      <span class="dt">xlab=</span><span class="st">&quot;temperature&quot;</span>, <span class="dt">main =</span><span class="st">&quot;KNN fit for riders ~ temperature, k = 40&quot;</span>)</span>
<span id="cb106-12"><a href="regresión-lineal.html#cb106-12"></a><span class="kw">lines</span>(<span class="dt">x=</span>est<span class="op">$</span>temp_rounded, <span class="dt">y=</span>est<span class="op">$</span>knn, <span class="dt">col=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>El sesgo es mayor aquí porque el error del modelo en la muestra es visiblemente mayor que en el caso de <span class="math inline">\(k\)</span>= 4, pero por esa misma razón es probable que la varianza sea menor. <em>No hay forma de lograr un sesgo bajo y una varianza baja simultáneamente.</em> Todo lo que puede hacer es tratar de equilibrar los dos, aceptando un sesgo moderado para lograr un mejor rendimiento fuera de la muestra. La técnica que usamos para lograr este equilibrio es la validación cruzada, que cubriremos más adelante en el curso. Por ahora podemos notar que el mejor valor para <span class="math inline">\(k\)</span> en la regresión KNN es el que minimiza la varianza, no el sesgo.</p>
<p>Como referencia, aquí hay un código para ajustar una regresión KNN usando el paquete <code>caret</code> en R. Usaremos<code>caret</code> frecuentemente en el curso porque proporciona una sintaxis consistente para ajustar una amplia gama de modelos (incluyendo, si quisiéramos, regresión lineal) y porque, muy convenientemente, ejecuta una validación cruzada en segundo plano para elegir parámetros de modelo óptimos como <span class="math inline">\(k\)</span>. Pero esto lo veremos más adelante.</p>
</div>
<div id="estimación-paramétrica-de-la-función-de-regresión-regresión-lineal" class="section level2" number="3.10">
<h2><span class="header-section-number">3.10</span> Estimación paramétrica de la función de regresión: regresión lineal</h2>
<p>Con la regresión KNN no asumimos nada sobre la forma de la función de regresión, sino que la estimamos directamente a partir de los datos. Supongamos ahora que <span class="math inline">\(\mu(t)\)</span> es lineal y se puede describir con los parámetros de una recta: <span class="math inline">\(\mu (t) = \beta_0 + \beta_1t\)</span>, donde <span class="math inline">\(\beta_0\)</span> es el <em>intercept</em> de la recta y <span class="math inline">\(\beta_1\)</span> es la pendiente. En este caso, entonces, <span class="math inline">\(\widehat{riders} = \beta_0 + \beta_1temperature\)</span>. Dado que <span class="math inline">\(\mu(t)\)</span> es una función poblacinal (es decir, promedio), los parámetros <span class="math inline">\(\beta_0\)</span> y <span class="math inline">\(\beta_1\)</span> son valores de población y son desconocidos, pero podemos estimarlos fácilmente.</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="regresión-lineal.html#cb107-1"></a><span class="kw">summary</span>(linear_fit &lt;-<span class="st"> </span><span class="kw">lm</span>(count <span class="op">~</span><span class="st"> </span>temperature, <span class="dt">data =</span> day))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = count ~ temperature, data = day)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4615.3 -1134.9  -104.4  1044.3  3737.8 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   1214.6      161.2   7.537 1.43e-13 ***
## temperature   6640.7      305.2  21.759  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1509 on 729 degrees of freedom
## Multiple R-squared:  0.3937,	Adjusted R-squared:  0.3929 
## F-statistic: 473.5 on 1 and 729 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="regresión-lineal.html#cb109-1"></a><span class="kw">ggplot</span>(day, <span class="kw">aes</span>(temperature, count)) <span class="op">+</span></span>
<span id="cb109-2"><a href="regresión-lineal.html#cb109-2"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb109-3"><a href="regresión-lineal.html#cb109-3"></a><span class="st">  </span><span class="kw">stat_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se=</span>F) <span class="op">+</span></span>
<span id="cb109-4"><a href="regresión-lineal.html#cb109-4"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;riders ~ temperature, mediante regresión lineal&quot;</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>Interpretemos estos coeficientes del modelo:</p>
<ul>
<li><p><strong>intercept</strong>: 1214.6 representa el número de pasajeros pronosticado cuando la temperatura es igual a 0. El <em>intercept</em> no es significativo porque la temperatura mínima en el conjunto de datos es . Podríamos hacerlo significativo al centrar la variable de temperatura en 0, en cuyo caso el <em>intercept</em> representaría el número de pasajeros promedio a la temperatura promedio. (Recuerde: las transformaciones lineales como el centrado no cambian el ajuste del modelo). La función <code>summary()</code> también genera un error estándar, valor t y valor p (“Pr (&gt; | t |)”) para el <em>intercept</em>ar. Explicaremos estos valores a continuación cuando revisemos la inferencia en el contexto de la regresión.</p></li>
<li><p><em>temperatura</em>: 6640.7 representa el cambio previsto en el número de pasajeros asociado con un aumento de la temperatura de 1 unidad. Desafortunadamente, este coeficiente, como el <em>intercept</em>, no es muy interpretable porque el rango de la variable de temperatura es solo -, lo que significa que la temperatura realmente puede no aumenta en 1 unidad. Podríamos aplicar aquí otra transformación lineal, para desnormalizar la temperatura, pero, nuevamente, esa transformación no cambiaría el ajuste: la pendiente de la recta de regresión permanecería igual.</p></li>
</ul>
<p>Ahora podemos preguntar: ¿cuál de estos dos modelos de pasajeros, el modelo paramétrico que usa regresión lineal o el modelo no paramétrico que usa KNN, es mejor? ¿Qué entendemos por mejor? Una respuesta a esa pregunta está en términos del ajuste en la muestra. ¿Cuál es el RMSE del modelo KNN en comparación con el RMSE del modelo lineal?</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="regresión-lineal.html#cb110-1"></a><span class="kw">rmse</span>(<span class="kw">predict</span>(linear_fit), day<span class="op">$</span>count)</span></code></pre></div>
<pre><code>## [1] 1507.322</code></pre>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="regresión-lineal.html#cb112-1"></a><span class="kw">rmse</span>(<span class="kw">predict</span>(knn_fit), day<span class="op">$</span>count)</span></code></pre></div>
<pre><code>## [1] 1321.889</code></pre>
<p>(<code>predict()</code> es equivalente a <code>adjust()</code> en este contexto.) Aquí podemos ver que el modelo KNN supera al modelo lineal en la muestra: en promedio, el modelo KNN está desfasado en aproximadamente 1322 ciclistas por día, mientras que el modelo lineal tiene una diferencia de 1507. Este tipo de comparación de modelos puede resultar muy útil. En este caso, sugiere que hay margen de mejora en el modelo lineal. Por un lado, podemos ver que la relación entre la temperatura y los pasajeros no es exactamente lineal: el número de pasajeros aumenta con la temperatura hasta aproximadamente .6, momento en el que se estabiliza y disminuye. La regresión KNN es mejor para modelar esta no linealidad. Sin embargo, podemos usar un modelo lineal para modelar un resultado no lineal agregando predictores.</p>
<p>Es probable que el número de pasajeros varíe bastante según la temporada. Agreguemos una variable por temporada para ver si mejora el modelo. Necesitamos definir la temporada explícitamente como un factor, lo cual podemos hacer dentro de la función <code>lm ()</code> usando <code>factor ()</code>. Esto es apropiado porque la estación no es una variable continua, sino un número entero que representa las diferentes estaciones y que toma solo cuatro valores: 1 - 4. R malinterpretará la estación como una variable continua a menos que la definamos explícitamente como un factor. El orden numérico de los valores de temporada definirá automáticamente los niveles. La función <code>lm ()</code> tratará el primer nivel, temporada = 1, como el nivel de referencia, con el que se compararán los otros niveles.</p>
<p>¿Cómo sabemos cuándo un predictor debe definirse como continuo y cuándo debe definirse como factor? Aquí hay una regla general: si restamos un nivel de otro y la diferencia tiene sentido, entonces podemos representar con seguridad esa variable como un número entero. Pensemos en la variable años de educación. La diferencia entre 10 años de escolaridad y 11 años es un año de educación, lo cual es una diferencia significativa. No estamos <em>obligados</em> a representar la educación como una variable continua, pero podríamos. (Codificar la educación como un factor esencialmente encajaría en una regresión separada para cada nivel, lo que podría correr el riesgo de sobreajuste.) Por el contrario, consideremos el código postal: una diferencia de 1 entre dos códigos postales de 5 dígitos no tiene sentido porque los códigos postales no tienen un orden intrínseco; representan diferencias categóricas, que nunca deben codificarse como números enteros. En cambio, estas variables siempre deben codificarse como factores. En este <a href="https://stats.idre.ucla.edu/other/mult-pkg/whatstat/what-is-the-difference-between-categorical-ordinal-and-variables-de-intervalo/">link</a> podéis encontrar más información al respecto.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="regresión-lineal.html#cb114-1"></a><span class="kw">summary</span>(linear_fit2 &lt;-<span class="st"> </span><span class="kw">lm</span>(count <span class="op">~</span><span class="st"> </span>temperature <span class="op">+</span><span class="st"> </span><span class="kw">factor</span>(season), <span class="dt">data =</span> day))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = count ~ temperature + factor(season), data = day)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4812.9  -996.8  -271.3  1240.9  3881.1 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)        745.8      187.5   3.978 7.65e-05 ***
## temperature       6241.3      518.1  12.046  &lt; 2e-16 ***
## factor(season)2    848.7      197.1   4.306 1.89e-05 ***
## factor(season)3    490.2      259.0   1.893   0.0588 .  
## factor(season)4   1342.9      164.6   8.159 1.49e-15 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1433 on 726 degrees of freedom
## Multiple R-squared:  0.4558,	Adjusted R-squared:  0.4528 
## F-statistic:   152 on 4 and 726 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="regresión-lineal.html#cb116-1"></a><span class="kw">rmse</span>(<span class="kw">predict</span>(linear_fit), day<span class="op">$</span>count)</span></code></pre></div>
<pre><code>## [1] 1507.322</code></pre>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="regresión-lineal.html#cb118-1"></a><span class="kw">rmse</span>(<span class="kw">predict</span>(linear_fit2), day<span class="op">$</span>count)</span></code></pre></div>
<pre><code>## [1] 1428.151</code></pre>
<p>El ajuste ha mejorado; el modelo con temporada (season) tiene un RMSE más bajo. Pero, ¿cómo interpretamos los coeficientes de una variable factor?. Observamos que sólo hay 3 coeficientes para 4 temporadas. ¿No debería haber 4 coeficientes? ¿Ha cometido un error la función <code>lm ()</code>? No. Para una variable factor como la temporada, cada coeficiente representa el cambio en la respuesta asociado con un cambio en el predictor desde el primer nivel o nivel de referencia a cada nivel de factor subsiguiente. (Esta codificación, la predeterminada en <code>lm ()</code>, se puede ajustar con el argumento <code>contrasts</code>). El nivel de referencia normalmente no se muestra en la salida del modelo. Si un predictor tiene <span class="math inline">\(k\)</span> niveles, entonces habrá <span class="math inline">\(k-1\)</span> coeficientes que representan los cambios previstos en el resultado asociados con aumentos desde el nivel de referencia en el predictor.</p>
<ul>
<li><p><em>factor(temporada)2</em>: 848,7 es el cambio previsto en los ciclistas de primavera con respecto al invierno (temporada = 1).</p></li>
<li><p><em>factor(temporada)3</em>: 490,2 es el cambio previsto en los ciclistas de verano, nuevamente respecto al invierno, que es la categoría de referencia.</p></li>
</ul>
<p>Los aumentos de temperatura pueden tener diferentes impactos en el número de ciclistas en diferentes estaciones. Podríamos probar esta hipótesis al incluir una interacción entre la estación y la temperatura.</p>
<p>La salida de la función <code>summary ()</code> puede volverse difícil de manejar. En su lugar, usaremos la función <code>display ()</code> del paquete arm, que ofrece un resumen del modelo más conciso.</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="regresión-lineal.html#cb120-1"></a><span class="kw">display</span>(linear_fit3 &lt;-<span class="st"> </span><span class="kw">lm</span>(count <span class="op">~</span><span class="st"> </span>temperature <span class="op">*</span><span class="st"> </span><span class="kw">factor</span>(season), <span class="dt">data =</span> day))</span></code></pre></div>
<pre><code>## lm(formula = count ~ temperature * factor(season), data = day)
##                             coef.est coef.se 
## (Intercept)                  -111.04   321.28
## temperature                  9119.04  1020.33
## factor(season)2              1513.43   571.76
## factor(season)3              6232.96  1079.33
## factor(season)4              2188.52   534.98
## temperature:factor(season)2 -2524.78  1326.47
## temperature:factor(season)3 -9795.26  1774.32
## temperature:factor(season)4 -2851.25  1414.93
## ---
## n = 731, k = 8
## residual sd = 1406.35, R-Squared = 0.48</code></pre>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="regresión-lineal.html#cb122-1"></a><span class="kw">rmse</span>(<span class="kw">predict</span>(linear_fit2), day<span class="op">$</span>count)</span></code></pre></div>
<pre><code>## [1] 1428.151</code></pre>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="regresión-lineal.html#cb124-1"></a><span class="kw">rmse</span>(<span class="kw">predict</span>(linear_fit3), day<span class="op">$</span>count)</span></code></pre></div>
<pre><code>## [1] 1398.634</code></pre>
<p>La interacción mejora el ajuste del modelo.</p>
<ul>
<li><p><em>temperatura: factor (temporada) 2</em>: -2524.8 representa la diferencia en la pendiente de la temperatura comparando la temporada 2 con la temporada 1. El coeficiente negativo significa que un aumento de 1 unidad la temperatura se asocia con un cambio * menor * en el número de pasajeros en primavera en comparación con el invierno.</p></li>
<li><p><em>temperatura: factor (temporada) 3</em>: -9795.3 representa la diferencia en la pendiente de la temperatura comparando la temporada 3 con la temporada 1 Y así sucesivamente.</p></li>
</ul>
<p>En un modelo con interacciones, debemos tener cuidado de interpretar los efectos principales con precisión.</p>
<ul>
<li><p><em>temperatura</em>: 9119 es el efecto principal de la temperatura y representa el cambio previsto en los ciclistas asociado con un aumento de 1 unidad en la temperatura cuando la temporada = 1 (la categoría de referencia) . En un modelo sin la interacción, el coeficiente de temperatura representaría el cambio promedio en los ciclistas asociado con un cambio de 1 unidad en la temperatura * manteniendo constante la temporada *.</p></li>
<li><p><em>factor (temporada) 2</em>: 1513.4 representa el cambio previsto en los ciclistas asociado con un aumento de 1 unidad en la temporada (es decir, de la temporada 1 a la temporada 2) cuando la temperatura = 0. Y así sucesivamente. Debido a que la temperatura no es igual a 0 en estos datos, los efectos principales de la temporada no son significativos.</p></li>
</ul>
<p>Para entender una interacción ¡es fundamental visualizarla! De hecho, en estadística deberíamos siempre empezar por esto antes de hacer inferencia</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="regresión-lineal.html#cb126-1"></a><span class="kw">ggplot</span>(day, <span class="kw">aes</span>(temperature, count)) <span class="op">+</span></span>
<span id="cb126-2"><a href="regresión-lineal.html#cb126-2"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb126-3"><a href="regresión-lineal.html#cb126-3"></a><span class="st">  </span><span class="kw">stat_smooth</span>(<span class="kw">aes</span>(<span class="dt">group =</span> <span class="kw">factor</span>(season), <span class="dt">col =</span> <span class="kw">factor</span>(season)), <span class="dt">method=</span><span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> F) <span class="op">+</span></span>
<span id="cb126-4"><a href="regresión-lineal.html#cb126-4"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Riders ~ temperature, según estación (season)&quot;</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>Aquí podemos ver que la relación entre la temperatura y los ciclistas es más fuertemente positiva (más pronunciada) en la temporada 1, más plana en las temporadas 2 y 4, y negativa en la temporada 3. Claramente, la temperatura tiene diferentes efectos en diferentes estaciones. En enero y febrero, un día más cálido provoca un gran aumento de ciclistas: el clima es mejor para ir en bici. En julio, un día más cálido provoca una disminución de ciclistas: el clima es “peor” para ir en bici (peor = cuesta más).</p>
<p>Los coeficientes del modelo lineal proporcionan una gran información sobre de los factores que influyen en el número de pasajeros. Sin embargo, nuestro modelo lineal todavía tiene un rendimiento inferior al modelo KNN.</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="regresión-lineal.html#cb127-1"></a><span class="kw">rmse</span>(<span class="kw">predict</span>(knn_fit), day<span class="op">$</span>count)</span></code></pre></div>
<pre><code>## [1] 1321.889</code></pre>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="regresión-lineal.html#cb129-1"></a><span class="kw">rmse</span>(<span class="kw">predict</span>(linear_fit3), day<span class="op">$</span>count)</span></code></pre></div>
<pre><code>## [1] 1398.634</code></pre>
<p>La regresión lineal a menudo tendrá un sesgo más alto que un método flexible como la regresión KNN, pero también tenderá a tener una varianza más baja. Exploraremos estas propiedades más a fondo cuando lleguemos a la validación cruzada.</p>
<p>Echemos un vistazo a la matriz del modelo.</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="regresión-lineal.html#cb131-1"></a><span class="kw">head</span>(<span class="kw">model.matrix</span>(linear_fit3))</span></code></pre></div>
<pre><code>##   (Intercept) temperature factor(season)2 factor(season)3 factor(season)4 temperature:factor(season)2 temperature:factor(season)3
## 1           1    0.344167               0               0               0                           0                           0
## 2           1    0.363478               0               0               0                           0                           0
## 3           1    0.196364               0               0               0                           0                           0
## 4           1    0.200000               0               0               0                           0                           0
## 5           1    0.226957               0               0               0                           0                           0
## 6           1    0.204348               0               0               0                           0                           0
##   temperature:factor(season)4
## 1                           0
## 2                           0
## 3                           0
## 4                           0
## 5                           0
## 6                           0</code></pre>
<p>Podemos ver que <code>lm()</code> ha convertido la variable de temporada en 3 vectores variables ficticias: factor (temporada) 2, factor (temporada) 3 y factor (temporada) 4. (Si un factor tiene niveles de $k $, entonces una variable ficticia para ese factor codifica $k - 1 $de esos niveles como variables binarias, con los valores 0 o 1 indicando la ausencia o presencia de ese nivel). Los términos de interacción consisten en los productos de los vectores componentes.</p>
</div>
<div id="predicción-usando-el-objeto-del-modelo" class="section level2" number="3.11">
<h2><span class="header-section-number">3.11</span> Predicción usando el objeto del modelo</h2>
<p>Podemos usar un modelo lineal no solo para la descripción sino también para la predicción. Si, por ejemplo, estuviéramos interesados en usar el modelo anterior para predecir el número de pasajeros para una temporada y temperatura en particular, digamos, un día caluroso en primavera, simplemente podríamos usar la ecuación de regresión. Definiremos un día caluroso como .85 (ojo con las escalas y las unidades de medida). Así:</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="regresión-lineal.html#cb133-1"></a><span class="dv">-111</span> <span class="op">+</span><span class="st"> </span><span class="dv">9119</span><span class="op">*</span>.<span class="dv">85</span> <span class="op">+</span><span class="st"> </span><span class="dv">1513</span><span class="op">*</span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">6233</span><span class="op">*</span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span><span class="dv">2189</span><span class="op">*</span><span class="dv">0</span> <span class="op">-</span><span class="st"> </span><span class="dv">2525</span><span class="op">*</span>.<span class="dv">85</span><span class="op">*</span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="dv">9795</span><span class="op">*</span>.<span class="dv">85</span><span class="op">*</span><span class="dv">0</span> <span class="op">-</span><span class="st"> </span><span class="dv">2851</span><span class="op">*</span>.<span class="dv">85</span><span class="op">*</span><span class="dv">0</span> </span></code></pre></div>
<pre><code>## [1] 7006.9</code></pre>
<p>Aquí hay una forma más precisa de hacer el cálculo que evita errores de redondeo al hacer referencia al objeto del modelo:</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="regresión-lineal.html#cb135-1"></a>t &lt;-<span class="st"> </span><span class="fl">.85</span></span>
<span id="cb135-2"><a href="regresión-lineal.html#cb135-2"></a>coefs &lt;-<span class="st"> </span><span class="kw">coef</span>(linear_fit3)</span>
<span id="cb135-3"><a href="regresión-lineal.html#cb135-3"></a>coefs[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>coefs[<span class="dv">2</span>]<span class="op">*</span>t <span class="op">+</span><span class="st"> </span>coefs[<span class="dv">3</span>]<span class="op">*</span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>coefs[<span class="dv">4</span>]<span class="op">*</span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span></span>
<span id="cb135-4"><a href="regresión-lineal.html#cb135-4"></a><span class="st">  </span>coefs[<span class="dv">5</span>]<span class="op">*</span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>coefs[<span class="dv">6</span>]<span class="op">*</span>t<span class="op">*</span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>coefs[<span class="dv">7</span>]<span class="op">*</span>t<span class="op">*</span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>coefs[<span class="dv">8</span>]<span class="op">*</span>t<span class="op">*</span><span class="dv">0</span>  </span></code></pre></div>
<pre><code>## (Intercept) 
##    7007.501</code></pre>
<p>Los resultados son diferentes debido al error de redondeo en el primer caso. El segundo método es más preciso.</p>
<p>Podemos hacer el mismo cálculo tratando el vector de coeficientes como una matriz y usando la multiplicación de matrices. Esto requiere menos escritura pero, al igual que con el método anterior, requiere prestar mucha atención al orden de los términos.</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="regresión-lineal.html#cb137-1"></a>coefs <span class="op">%*%</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, t, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, t<span class="op">*</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span></code></pre></div>
<pre><code>##          [,1]
## [1,] 7007.501</code></pre>
<p>Lo más simple de todo es definir un marco de datos con nuestros valores deseados y usar <code>predecir ()</code>:</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="regresión-lineal.html#cb139-1"></a><span class="kw">predict</span>(linear_fit3, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">season =</span> <span class="dv">2</span>, <span class="dt">temperature =</span> <span class="fl">.85</span>))</span></code></pre></div>
<pre><code>##        1 
## 7007.501</code></pre>
</div>
<div id="inferencia-en-el-contexto-de-regresión" class="section level2" number="3.12">
<h2><span class="header-section-number">3.12</span> Inferencia en el contexto de regresión</h2>
<p>Además de las estimaciones de coeficientes para cada variable predictora (incluido el <em>intercept</em>), la salida de <code>lm ()</code> (usando <code>summary ()</code>) contiene lo siguiente: “Error estándar”, “valor t” y “Pr (&gt; | t |)” (el valor p). Repasemos estos conceptos.</p>
<p>Recuerde que la inferencia estadística nos permite estimar las características de la población a partir de las propiedades de una muestra. Por lo general, queremos saber si una diferencia o una relación que observamos en una muestra es verdadera en la población — es “estadísticamente significativa” — o es probable que se deba al azar. En el contexto de la regresión, queremos saber específicamente si la pendiente de la recta de regresión, <span class="math inline">\(\beta\)</span>, que resume la relación de una variable con el resultado es diferente de 0. ¿Existe una relación positiva o negativa? En el paradigma frecuentista, respondemos a esta pregunta utilizando pruebas estadísticas basadas en test de hipótesis.</p>
<p>De otros cursos sabemos que una prueba de hipótesis se basa en plantear una “hipótesis nula”, <span class="math inline">\(H_0\)</span>. En la regresión, <span class="math inline">\(H_0\)</span> corresponde a que la pendiente de la recta de regresión, <span class="math inline">\(\beta\)</span>, es 0. Una pendiente de 0 significa que un predictor no tiene efecto o no tiene relación con el resultado. <code>R</code> calcula automáticamente una prueba de hipótesis para <span class="math inline">\(\beta\)</span> usando el estadístico t, definido como:</p>
<p><span class="math display">\[
t = \frac {\beta - 0} {SE (\beta)}
\]</span></p>
<p>El estadístico <span class="math inline">\(t\)</span> para una muestra sigue la distribución <span class="math inline">\(t\)</span> de Student con n - 2 grados de libertad. Para la regresión lineal multivariante, el estadístico <span class="math inline">\(t\)</span>sigue la distribución <span class="math inline">\(t\)</span> de Student con $n - k - 1 $ grados de libertad, donde <span class="math inline">\(k\)</span> representa el número de predictores en el modelo. Se utiliza la distribución <span class="math inline">\(t\)</span> porque es más conservadora que una distribución normal cuando <span class="math inline">\(n\)</span> es pequeño ya que en ese caso no podemos asumir el teorema central del límite que nos permitiría determinar que la distribución del estadístico sigue una distribución normal. La distribución <span class="math inline">\(t\)</span> de Student tiene una cola más pesada pero converge a la normal cuando <span class="math inline">\(n\)</span> aumenta (por encima de aproximadamente <span class="math inline">\(n\)</span>= 30). Por otro lado, <span class="math inline">\(SE (\beta)\)</span> se define como</p>
<p><span class="math display">\[
SE (\beta) = \frac {RSE} {\sqrt {\sum_{i = 1} ^ n (x_i - \bar {x} _i)}},
\]</span></p>
<p>donde error estándar residual (RSE) se calcula como</p>
<p><span class="math display">\[
RSE = \sqrt {\frac {RSS} {n - 2}},
\]</span></p>
<p>y la suma de cuadrados residual (RSS) se puede definir como (en una formulación ligeramente diferente a la que hemos usado antes):</p>
<p><span class="math display">\[
RSS = \sum_ {i = 1} ^ n (y_i - f (x_i)) ^ 2.
\]</span></p>
<p>Después de calcular el estadístico t, usamos una prueba t para compararlo con el valor crítico dado un nivel de significación que suele ser del 5% para la distribución <span class="math inline">\(t\)</span> con n - 2 grados de libertad.</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="regresión-lineal.html#cb141-1"></a><span class="kw">summary</span>(linear_fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = count ~ temperature, data = day)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4615.3 -1134.9  -104.4  1044.3  3737.8 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   1214.6      161.2   7.537 1.43e-13 ***
## temperature   6640.7      305.2  21.759  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1509 on 729 degrees of freedom
## Multiple R-squared:  0.3937,	Adjusted R-squared:  0.3929 
## F-statistic: 473.5 on 1 and 729 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="regresión-lineal.html#cb143-1"></a>rse &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>((day<span class="op">$</span>count <span class="op">-</span><span class="st"> </span><span class="kw">predict</span>(linear_fit))<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span>(<span class="kw">nrow</span>(day) <span class="op">-</span><span class="st"> </span><span class="dv">2</span>))</span>
<span id="cb143-2"><a href="regresión-lineal.html#cb143-2"></a>(seb &lt;-<span class="st"> </span>rse<span class="op">/</span><span class="kw">sqrt</span>(<span class="kw">sum</span>((day<span class="op">$</span>temperature <span class="op">-</span></span>
<span id="cb143-3"><a href="regresión-lineal.html#cb143-3"></a><span class="st">                        </span><span class="kw">mean</span>(day<span class="op">$</span>temperature))<span class="op">^</span><span class="dv">2</span>)))</span></code></pre></div>
<pre><code>## [1] 305.188</code></pre>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="regresión-lineal.html#cb145-1"></a>(t &lt;-<span class="st"> </span><span class="kw">as.numeric</span>((<span class="kw">coef</span>(linear_fit)[<span class="dv">2</span>] <span class="op">-</span><span class="st"> </span><span class="dv">0</span>) <span class="op">/</span><span class="st"> </span>seb))</span></code></pre></div>
<pre><code>## [1] 21.75941</code></pre>
<p>Nuestro cálculo coincide exactamente con la salida del modelo lineal.</p>
<p>Grafiquemos este estadístico t contra la distribución nula de una t de Student con 729 grados de libertad. Usamos la función <code>dt ()</code> para generar una gráfica de densidad para una distribución t con 729 grados de libertad, y <code>qt ()</code> para identificar los valores críticos para un IC del 95% en la distribución nula; los valores con baja probabilidad (p &lt;.05) estarán a la izquierda del IC inferior oa la derecha del IC superior. Los valores P y los IC proporcionan la misma información sobre lo inusual de un valor observado bajo el nulo.</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="regresión-lineal.html#cb147-1"></a>tdist &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>, <span class="dv">25</span>, <span class="fl">.01</span>), <span class="dt">y =</span> <span class="kw">dt</span>(<span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>, <span class="dv">25</span>, <span class="fl">.01</span>), <span class="dt">df =</span> <span class="dv">729</span>))</span>
<span id="cb147-2"><a href="regresión-lineal.html#cb147-2"></a></span>
<span id="cb147-3"><a href="regresión-lineal.html#cb147-3"></a><span class="kw">qt</span>(<span class="kw">c</span>(.<span class="dv">025</span>, <span class="fl">.975</span>), <span class="dt">df =</span> <span class="dv">729</span>)</span></code></pre></div>
<pre><code>## [1] -1.963223  1.963223</code></pre>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="regresión-lineal.html#cb149-1"></a><span class="kw">ggplot</span>(tdist, <span class="kw">aes</span>(x, y)) <span class="op">+</span></span>
<span id="cb149-2"><a href="regresión-lineal.html#cb149-2"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb149-3"><a href="regresión-lineal.html#cb149-3"></a><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> t, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lty =</span> <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb149-4"><a href="regresión-lineal.html#cb149-4"></a><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">qt</span>(.<span class="dv">025</span>, <span class="dt">df =</span> <span class="dv">729</span>), <span class="dt">lty =</span> <span class="dv">2</span>) <span class="op">+</span></span>
<span id="cb149-5"><a href="regresión-lineal.html#cb149-5"></a><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">qt</span>(.<span class="dv">975</span>, <span class="dt">df =</span> <span class="dv">729</span>), <span class="dt">lty =</span> <span class="dv">2</span>) <span class="op">+</span></span>
<span id="cb149-6"><a href="regresión-lineal.html#cb149-6"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;t(n - 2 = 729) según valores críticos (negro) y el estadístico t observado (rojo)&quot;</span>) <span class="op">+</span></span>
<span id="cb149-7"><a href="regresión-lineal.html#cb149-7"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;t-statistic&quot;</span>) <span class="op">+</span></span>
<span id="cb149-8"><a href="regresión-lineal.html#cb149-8"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;density&quot;</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<p>Un estadístico t de 21.76 esencialmente nunca ocurriría bajo la distribución nula, lo que nos permite “rechazar <span class="math inline">\(H_0\)</span>” con un nivel de confianza del 95% (suponiendo un nivel de significación del 5%). El valor p asociado con el coeficiente <span class="math inline">\(\beta\)</span> para la temperatura en el resumen del modelo — esencialmente cero — refleja este resultado. Podemos calcular nuestro propio valor p con el siguiente código:</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="regresión-lineal.html#cb150-1"></a><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">pt</span>(t, <span class="dt">df =</span> <span class="dv">729</span>, <span class="dt">lower.tail =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<pre><code>## [1] 2.810622e-81</code></pre>
<p>Usamos <code>lower.tail = F</code> porque estamos interesados en la probabilidad de t =<code>r round (as.numeric (t), 2)</code>en la cola <em>superior</em> además esta forma es más informativa que hacerlo con:</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="regresión-lineal.html#cb152-1"></a><span class="dv">1</span> <span class="op">-</span><span class="st">  </span><span class="kw">pt</span>(t, <span class="dt">df =</span> <span class="dv">729</span>)</span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<p>que nos daría un p-valor de 0 por un problema de tolerancia de nuestra máquina (del orden de <span class="math inline">\(10^{-21}\)</span>)</p>
<p>El resumen del modelo también genera otro estadístico basado en la distribución F con un valor p asociado:</p>
<p><span class="math display">\[
F = \frac{\frac{TSS - RSS}{p - 1}}{\frac{RSS}{n - p}}
\]</span></p>
<p>La hipótesis nula para esta prueba F es: <span class="math inline">\(H_0: \beta_1 = ... \beta_ {p-1} = 0\)</span>. En otras palabras, la prueba responde a la pregunta: “¿Alguno de los predictores es útil para predecir la respuesta?” Esta no es una medida muy útil del rendimiento del modelo, ya que los modelos casi siempre tienen <em>algún</em> valor predictivo.</p>
<p>NOTA (Avanzado por si lo necesitáis en el futuro): Podríamos estimar muy fácilmente <span class="math inline">\(SE(\beta)\)</span> usando bootstrap (<a href="https://uc-r.github.io/bootstrapping">aquí</a> tenéis una descripción de este método). Utilizaremos este enfoque si tenemos motivos para desconfiar de cómo se calcula <span class="math inline">\(SE(\beta)\)</span> analíticamente usando <code>lm ()</code>. Por ejemplo, en el caso de errores heterocedásticos (discutidos a continuación) <code>lm ()</code> tenderá a subestimar <span class="math inline">\(SE(\beta)\)</span> y tendríamos muchos resultados significativos que serían falsos. Obtener resultados similares utilizando boostrap nos haría confiar en los resultados reportados por <code>lm ()</code>.</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="regresión-lineal.html#cb154-1"></a>temperature_coef &lt;-<span class="st"> </span><span class="ot">NULL</span></span>
<span id="cb154-2"><a href="regresión-lineal.html#cb154-2"></a></span>
<span id="cb154-3"><a href="regresión-lineal.html#cb154-3"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">1000</span>){</span>
<span id="cb154-4"><a href="regresión-lineal.html#cb154-4"></a>  rows &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(day), <span class="dt">replace =</span> T)</span>
<span id="cb154-5"><a href="regresión-lineal.html#cb154-5"></a>  boot_sample &lt;-<span class="st"> </span>day[rows, ]</span>
<span id="cb154-6"><a href="regresión-lineal.html#cb154-6"></a>  model &lt;-<span class="st"> </span><span class="kw">lm</span>(count <span class="op">~</span><span class="st"> </span>temperature, <span class="dt">data =</span> boot_sample)</span>
<span id="cb154-7"><a href="regresión-lineal.html#cb154-7"></a>  temperature_coef[i] &lt;-<span class="st"> </span><span class="kw">coef</span>(model)[<span class="dv">2</span>]</span>
<span id="cb154-8"><a href="regresión-lineal.html#cb154-8"></a>}</span>
<span id="cb154-9"><a href="regresión-lineal.html#cb154-9"></a></span>
<span id="cb154-10"><a href="regresión-lineal.html#cb154-10"></a><span class="kw">sd</span>(temperature_coef)</span></code></pre></div>
<pre><code>## [1] 280.7121</code></pre>
<p>En este caso, la estimación de <span class="math inline">\(SE (\beta_{temp})\)</span> es similar pero menor que la calculada analíticamente. Por lo tanto, la estimación <code>lm()</code> es en realidad más conservadora en este caso.</p>
<p>El <span class="math inline">\(SE\)</span> para los coeficientes en la salida de <code>lm ()</code> se puede usar para calcular los IC para la estimación del coeficiente. <span class="math inline">\(\hat {\beta} + 1.96 (SE)\)</span> nos da el límite superior al 95%, y $ - 1.96 (SE) $ el límite inferior. Los IC del 95% que no incluyen 0 indican que el coeficiente es estadísticamente significativo, equivalente a un valor p del coeficiente menor de .05.</p>
<p>El uso de IC en lugar de p-valores nos da una forma más flexible de hacer inferencia. Además, también suele ser útil porque nos indica qué posibles valores puede tomar nuestros parámetros (si quisiéramos cuantificar el efecto). Ésta es otra razón para usar la función <code>display ()</code> del paquete arm. No solo presenta la salida de <code>lm ()</code> de manera más compacta, sino que tampoco reporta estadísticas t ni valores p. Como hemos visto, <span class="math inline">\(SE\)</span>s transmiten la misma información.</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="regresión-lineal.html#cb156-1"></a><span class="kw">display</span>(linear_fit)</span></code></pre></div>
<pre><code>## lm(formula = count ~ temperature, data = day)
##             coef.est coef.se
## (Intercept) 1214.64   161.16
## temperature 6640.71   305.19
## ---
## n = 731, k = 2
## residual sd = 1509.39, R-Squared = 0.39</code></pre>
<p>Los valores posibles (recordamos que el verdadero valor del parámetro es desconocido en la población) para la temperatura son <span class="math inline">\(6641 \pm 2(305)\)</span> o aproximadamente [6031,<code>r 6641 + 2 * 305</code>].<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> Este IC no incluye 0, de lo cual podemos concluir que la temperatura se asocia con el resultado de forma estadísticamente significativa. (El valor p para la temperatura informado en el <code>summary()</code> concuerda).</p>
</div>
<div id="asunciones-de-un-modelo-de-regresión" class="section level2" number="3.13">
<h2><span class="header-section-number">3.13</span> Asunciones de un modelo de regresión</h2>
<p>Los resultados de la regresión solo son precisos si se dan un conjunto de supuestos (en orden de importancia):<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<ol style="list-style-type: decimal">
<li><p><em>Validez de los datos</em> para responder a la pregunta de investigación.</p></li>
<li><p><em>Linealidad de la relación</em> entre el resultado y las variables predictoras.</p></li>
<li><p><em>Independencia de los errores</em> (en particular, sin correlación entre errores consecutivos como en el caso de los datos de series de tiempo).</p></li>
<li><p><em>Varianza igual de errores</em> (homocedasticidad).</p></li>
<li><p><em>Normalidad de errores.</em></p></li>
</ol>
<p>La mayoría de estos problemas no son fatales y se pueden solucionar mejorando el modelo, seleccionando variables diferentes o adicionales o utilizando una distribución de modelización diferente (los conocidos como modelos lineales generalizados o GLMs). Los gráficos de residuos son la mejor herramienta para evaluar si se han cumplido los supuestos del modelo.</p>
<p><em>1. Validez de los datos para responder a la pregunta de investigación </em></p>
<p>Esto puede parecer obvio pero es necesario enfatizarlo:</p>
<ul>
<li><p>La medida de resultado debe reflejar con precisión el fenómeno de interés.</p></li>
<li><p>El modelo debe incluir todas las variables relevantes.</p></li>
<li><p>El modelo debe generalizarse a todos los casos a los que se aplica.</p></li>
</ul>
<p>En resumen, debemos asegurarnos de que nuestros datos proporcionan información <em>precisa</em> y <em>relevante</em> para responder a la pregunta de investigación.</p>
<p><em>2. Supuesto de linealidad</em></p>
<p>La suposición matemática más importante del modelo de regresión es que el resultado es una función lineal determinista de los predictores separados: <span class="math inline">\(y = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2}...\)</span>. Podemos comprobar este supuesto visualmente trazando las variables predictoras contra el resultado:</p>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="regresión-lineal.html#cb158-1"></a><span class="kw">ggplot</span>(day, <span class="kw">aes</span>(temperature, count)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb158-2"><a href="regresión-lineal.html#cb158-2"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb158-3"><a href="regresión-lineal.html#cb158-3"></a><span class="st">  </span><span class="kw">stat_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> F) <span class="op">+</span></span>
<span id="cb158-4"><a href="regresión-lineal.html#cb158-4"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;count ~ temperature&quot;</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
<p>Los datos son claramente <em>no</em> lineales, ¿qué hacemos? Podemos agregar predictores al modelo, como la temporada, que permiten que un modelo <em>lineal</em> se ajuste mejor a datos <em>no</em> lineales. También podríamos considerar añadir un término cuadrático al modelo: conteo ~ temperatura + temperatura<span class="math inline">\(^2\)</span>.</p>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="regresión-lineal.html#cb159-1"></a>linear_fit4 &lt;-<span class="st"> </span><span class="kw">lm</span>(count <span class="op">~</span><span class="st"> </span>temperature <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(temperature<span class="op">^</span><span class="dv">2</span>), <span class="dt">data =</span> day)</span>
<span id="cb159-2"><a href="regresión-lineal.html#cb159-2"></a><span class="kw">ggplot</span>(day, <span class="kw">aes</span>(temperature, count)) <span class="op">+</span></span>
<span id="cb159-3"><a href="regresión-lineal.html#cb159-3"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb159-4"><a href="regresión-lineal.html#cb159-4"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(temperature, <span class="kw">fitted</span>(linear_fit4)), <span class="dt">col=</span> <span class="st">&quot;red&quot;</span>) <span class="op">+</span></span>
<span id="cb159-5"><a href="regresión-lineal.html#cb159-5"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;count ~ temperature + temperature^2&quot;</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-42-1.png" width="672" /></p>
<p>Una vez que se han añadido predictores adicionales, podemos verificar los gráficos de residuos, ya que si el modelo no cumple la condición de linealidad se mostrará en los residuos. <code>plot ()</code> es una función R incorporada para verificar la distribución de errores de un modelo.</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="regresión-lineal.html#cb160-1"></a><span class="kw">plot</span>(linear_fit, <span class="dt">which =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-43-1.png" width="672" /></p>
<p>Podemos ver, como era de esperar, una no linealidad significativa en la gráfica residual para el modelo con temperatura solamente. Esperamos que los residuos no tengan una estructura visible, ningún patrón. En términos del supuesto de linealidad, la línea de resumen roja no debe tener curvatura. Cuando agregamos temporada, la parcela residual, aunque no es perfecta, mejora mucho.</p>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb161-1"><a href="regresión-lineal.html#cb161-1"></a><span class="kw">plot</span>(linear_fit2, <span class="dt">which =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<p>Sin embargo, el modelo todavía lucha con días de gran volumen, con más de 5000 ciclistas previstos. Veamos si agregar una interacción entre la estación y la temperatura ayuda:</p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="regresión-lineal.html#cb162-1"></a><span class="kw">plot</span>(linear_fit3, <span class="dt">which =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-45-1.png" width="672" /></p>
<p>Quizás esto sea mejor. La no linealidad implica menos observaciones, la mayoría de ellas en los días con valores ajustados superiores a 6000. Pero ha surgido otro problema con este modelo: los errores heterocedásticos. Discutiremos este supuesto antes de la independencia de errores.</p>
<p><em>4. Igual varianza de errores</em> (homocedasticidad)</p>
<p>Observamos cómo los errores en el gráfico residual anterior tienen forma de embudo. Los errores del modelo no se distribuyen por igual en el rango de los valores ajustados, una situación conocida como heterocedasticidad. Una solución es transformar la variable de resultado tomando el registro (solo funciona con valores positivos). Si esto no funciona, recuerde que la principal consecuencia de los errores heterocedásticos es que los <span class="math inline">\(SE (\beta)\)</span>s son más pequeños de lo que deberían ser, lo que lleva a valores p más significativos de los que debería haber. Un remedio para este problema inferencial es calcular los errores estándar ajustados que son robustos a la varianza desigual; el paquete MASS ofrece la función <code>rlm ()</code> para ajustar tal modelo.</p>
<p>Aquí está la gráfica residual para un modelo de log (recuento):</p>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb163-1"><a href="regresión-lineal.html#cb163-1"></a><span class="kw">plot</span>(<span class="kw">lm</span>(<span class="kw">log</span>(count) <span class="op">~</span><span class="st"> </span>temperature <span class="op">*</span><span class="st"> </span>season, <span class="dt">data =</span> day), <span class="dt">which =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<p>Es posible que la heterocedasticidad haya mejorado, pero ahora han surgido algunos valores atípicos y todavía tenemos un problema de no linealidad. ¿Qué hacemos?</p>
<p>Después de revisar estos gráficos de residuos, deberíamos dar un paso atrás y pensar en nuestros datos. Un problema queda claro. Es probable que el número de pasajeros en días consecutivos sea muy similar debido a la temperatura, el clima y la temporada. En consecuencia, los residuos del modelo se agruparán (veríamos clusters). Si el modelo no hace un buen trabajo al contabilizar el número de pasajeros en, digamos, días de alta temperatura, los errores grandes no se distribuirán al azar sino que ocurrirán juntos, producidos por una ola de calor en julio, por ejemplo. Los errores de los días siguientes serán similares. En estos casos, la regresión lineal no sería un buen modelo ya que el supuesto de independencia de errores no se cumpliría. Estos problemas ocurren en la mayoría de casos que nuestros datos se recogen de forma seriada (series temporales). Es por ello que en estos casos se debe de utilizar otros modelos más complejos como la regresión KNN (entre otros). Sin embargo, debemos tener en cuenta que la regresión KNN puede ajustarse mejor a los datos y posiblemente ofrecer mejores predicciones, pero no ofrece ninguna ayuda para comprender las relaciones entre las variables y muchas veces (sobre todo en medicina) esto es muy importante. La regresión lineal, incluso si el modelo no es perfecto, proporciona información sobre los factores que afectan a la cantidad de usuarios, información que puede ser extremadamente valiosa, por ejemplo, para los administradores del programa de bicicletas compartidas, mientras que la regresión KNN solo puede ofrecer una predicción.</p>
<p><em>3. No independencia de errores (residuales correlacionados)</em></p>
<p>La falta de independencia de los errores ocurre en los datos de series de tiempo o en los datos con observaciones agrupadas, cuando, por ejemplo, varios puntos de datos provienen de individuos de un mismo barrio, país, , tiendas o aulas. Podemos diagnosticar los residuos correlacionados en los datos de los usuarios de bicicletas mirando un gráfico de residuos por fecha.</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="regresión-lineal.html#cb164-1"></a><span class="kw">data.frame</span>(<span class="dt">day =</span> <span class="kw">seq</span>(<span class="dv">1</span>,<span class="kw">nrow</span>(day)),</span>
<span id="cb164-2"><a href="regresión-lineal.html#cb164-2"></a>             <span class="dt">residuals =</span> <span class="kw">residuals</span>(linear_fit3)) <span class="op">%&gt;%</span></span>
<span id="cb164-3"><a href="regresión-lineal.html#cb164-3"></a><span class="kw">ggplot</span>(<span class="kw">aes</span>(day, residuals)) <span class="op">+</span></span>
<span id="cb164-4"><a href="regresión-lineal.html#cb164-4"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb164-5"><a href="regresión-lineal.html#cb164-5"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Residuales según el día: count ~ temperature * season&quot;</span>) <span class="op">+</span></span>
<span id="cb164-6"><a href="regresión-lineal.html#cb164-6"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">lty =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-47-1.png" width="672" /></p>
<p>Podemos ver muy claramente que los errores ocurren en grupos relacionados con la fecha. Quizás el patrón más importante venga dado por la variable año. Sin una variable que determine el año, el modelo tiene problemas ya que predice de forma insuficiente en el primer año y prediciendo de más en el segundo. Si agregamos un año al modelo, los residuos se ven mejor pero la agrupación sigue siendo evidente.</p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb165-1"><a href="regresión-lineal.html#cb165-1"></a><span class="kw">data.frame</span>(<span class="dt">day =</span> <span class="kw">seq</span>(<span class="dv">1</span>,<span class="kw">nrow</span>(day)),</span>
<span id="cb165-2"><a href="regresión-lineal.html#cb165-2"></a>             <span class="dt">residuals =</span> <span class="kw">residuals</span>(<span class="kw">update</span>(linear_fit3, <span class="op">~</span><span class="st"> </span>. <span class="op">+</span><span class="st"> </span>year))) <span class="op">%&gt;%</span></span>
<span id="cb165-3"><a href="regresión-lineal.html#cb165-3"></a><span class="kw">ggplot</span>(<span class="kw">aes</span>(day, residuals)) <span class="op">+</span></span>
<span id="cb165-4"><a href="regresión-lineal.html#cb165-4"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb165-5"><a href="regresión-lineal.html#cb165-5"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Residuales según el día: count ~ temperature * season + year&quot;</span>) <span class="op">+</span></span>
<span id="cb165-6"><a href="regresión-lineal.html#cb165-6"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">lty =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-48-1.png" width="672" /></p>
<p>¿Cómo abordamos los errores no independientes? Si la no independencia está relacionada con el tiempo, entonces deberíamos usar un modelo apropiado para datos de series de tiempo, como ARIMA (que se podrá ver en otras asignaturas). Si la agrupación se debe a alguna otra estructura en los datos, por ejemplo, la agrupación debido a la ubicación, entonces podríamos considerar el uso de un modelo jerárquico o multinivel (también se podrá ver en otras asignaturas). Para manejar errores no independientes con un modelo lineal, necesitamos agregar variables que controlen el agrupamiento. La agrupación en este caso se debe a la estacionalidad, por lo que agregamos predictores como año, temporada, mes o día de la semana. Si el modelo resultante aún no se ajusta bien a los datos y solo nos interesa la predicción, entonces podríamos considerar el uso de un modelo no paramétrico como KNN.</p>
<p><em>5. Normalidad de los residuales</em></p>
<p>Comparado con los otros supuestos, este no es muy importante. La regresión lineal es extremadamente robusta a las violaciones de la normalidad. Podemos comprobar visualmente la normalidad de los residuales con un histograma:</p>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="regresión-lineal.html#cb166-1"></a><span class="kw">data.frame</span>(<span class="dt">residuals =</span> <span class="kw">residuals</span>(linear_fit3)) <span class="op">%&gt;%</span></span>
<span id="cb166-2"><a href="regresión-lineal.html#cb166-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(residuals)) <span class="op">+</span></span>
<span id="cb166-3"><a href="regresión-lineal.html#cb166-3"></a><span class="kw">geom_histogram</span>() <span class="op">+</span></span>
<span id="cb166-4"><a href="regresión-lineal.html#cb166-4"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Residuales:  count ~ temperature * season&quot;</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-49-1.png" width="672" /></p>
<p>Queda bastante claro que el modelo sin año no es normal. La bimodalidad de esta distribución ofrece una pista de que el año es un término estacional clave. La librería car incluye una función, <code>qqPlot ()</code> que “muestra cuantiles empíricos de una variable, o de residuales studentizados de un modelo lineal, contra cuantiles teóricos de una distribución teórica con la que podemos comparar”.</p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="regresión-lineal.html#cb167-1"></a><span class="kw">library</span>(car)</span>
<span id="cb167-2"><a href="regresión-lineal.html#cb167-2"></a><span class="kw">qqPlot</span>(linear_fit3, <span class="dt">pch =</span> <span class="dv">20</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-50-1.png" width="672" /></p>
<pre><code>## [1] 239 668</code></pre>
<p>Aquí podemos ver desviaciones de la normalidad que también tienen una estructura anual discernible.</p>
<p>En resumen, utilizamos gráficos de residuos para validar y mejorar el ajuste del modelo. Si bien hay funciones disponibles para probar formalmente la mayoría de los supuestos del modelo anterior, es mejor (en mi opinión) evitar tales pruebas binarias a favor de graficar los residuos y pensar en los datos y cómo mejorar un modelo.</p>
</div>
<div id="ejemplos-adicionales-de-interpretación-de-modelos" class="section level2" number="3.14">
<h2><span class="header-section-number">3.14</span> Ejemplos adicionales de interpretación de modelos</h2>
<p>Para estos ejemplos, usaremos el conjunto de datos de vivienda de Boston, que registra los precios de la vivienda en el área de Boston en la década de 1970 junto con varios predictores. La variable de resultado es el valor mediano de las viviendas ocupadas por sus propietarios en $1000, codificado como “medv”. Los predictores incluyen lo siguiente:</p>
<ul>
<li><p>chas: variable ficticia de Charles River (= 1 si el tramo limita con el río; 0 en caso contrario).</p></li>
<li><p>lstat: menor estatus de la población (porcentaje). El diccionario de datos no es explícito, pero esta variable parece ser una medida del estatus socioeconómico de un barrio, representado como el porcentaje de clase trabajadora o familias pobres. Centraremos esta variable para que los efectos principales sean interpretables.</p></li>
<li><p>rm: número medio de habitaciones por vivienda en un área geográfica determinada. Centraremos esta variable para que los efectos principales sean interpretables.</p></li>
</ul>
<div id="interpretación-del-intercept-y-los-coeficientes-beta-para-un-modelo-con-predictores-continuos" class="section level3" number="3.14.1">
<h3><span class="header-section-number">3.14.1</span> Interpretación del <em>intercept</em> y los coeficientes <span class="math inline">\(\beta\)</span> para un modelo con predictores continuos</h3>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="regresión-lineal.html#cb169-1"></a><span class="kw">library</span>(MASS)</span>
<span id="cb169-2"><a href="regresión-lineal.html#cb169-2"></a><span class="kw">data</span>(Boston)</span>
<span id="cb169-3"><a href="regresión-lineal.html#cb169-3"></a>Boston<span class="op">$</span>rm_centered  &lt;-<span class="st"> </span>Boston<span class="op">$</span>rm <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(Boston<span class="op">$</span>rm)</span>
<span id="cb169-4"><a href="regresión-lineal.html#cb169-4"></a>Boston<span class="op">$</span>lstat_centered &lt;-<span class="st">  </span>Boston<span class="op">$</span>lstat <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(Boston<span class="op">$</span>lstat)</span>
<span id="cb169-5"><a href="regresión-lineal.html#cb169-5"></a><span class="kw">display</span>(<span class="kw">lm</span>(medv <span class="op">~</span><span class="st"> </span>rm_centered <span class="op">+</span><span class="st"> </span>lstat_centered, <span class="dt">data =</span> Boston))</span></code></pre></div>
<pre><code>## lm(formula = medv ~ rm_centered + lstat_centered, data = Boston)
##                coef.est coef.se
## (Intercept)    22.53     0.25  
## rm_centered     5.09     0.44  
## lstat_centered -0.64     0.04  
## ---
## n = 506, k = 3
## residual sd = 5.54, R-Squared = 0.64</code></pre>
<ul>
<li><p>La intersección con el eje Y (<em>intercept</em>) es el valor predicho de la variable de resultado cuando los predictores son 0. A veces, la intersección no será interpretable porque un predictor no puede = 0. La solución es centrar la variable para que 0 tenga sentido.</p></li>
<li><p><strong>intercept</strong>: El valor predicho de medv cuando todos los predictores son 0: 22.53 + 5.09 (0) - .64 (0).</p></li>
<li><p><em>rm_centered</em>: 5.09 representa el cambio predicho en medv cuando rm_centered aumenta en una unidad (1 habitación), mientras se mantienen constantes las otras variables.</p></li>
<li><p><em>lstat_centered</em>: -.64 representa el cambio predicho en medv cuando lstat_centered aumenta en una unidad, mientras se mantienen constantes las otras variables.</p></li>
</ul>
</div>
<div id="interpretación-del-intercept-y-los-coeficientes-beta-para-un-modelo-con-predictores-binarios-y-continuos" class="section level3" number="3.14.2">
<h3><span class="header-section-number">3.14.2</span> Interpretación del <em>intercept</em> y los coeficientes <span class="math inline">\(\beta\)</span> para un modelo con predictores binarios y continuos</h3>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="regresión-lineal.html#cb171-1"></a><span class="kw">display</span>(<span class="kw">lm</span>(medv <span class="op">~</span><span class="st"> </span>rm_centered <span class="op">+</span><span class="st"> </span>lstat_centered <span class="op">+</span><span class="st"> </span>chas, <span class="dt">data =</span> Boston))</span></code></pre></div>
<pre><code>## lm(formula = medv ~ rm_centered + lstat_centered + chas, data = Boston)
##                coef.est coef.se
## (Intercept)    22.25     0.25  
## rm_centered     4.96     0.44  
## lstat_centered -0.64     0.04  
## chas            4.12     0.96  
## ---
## n = 506, k = 4
## residual sd = 5.45, R-Squared = 0.65</code></pre>
<ul>
<li><p>La intersección (<em>intercept</em>) es el valor predicho de la variable de resultado cuando el predictor binario es 0 y la variable continua es 0 (que, para las variables centradas, es el promedio).</p></li>
<li><p><strong>intercept</strong>: El valor predicho de medv cuando todos los predictores son 0: 22.25 + 4.96 (0) -.64 (0) + 4.12 (0).</p></li>
<li><p><em>rm_centered</em>: 4.96 representa el cambio predicho en medv cuando rm_centered aumenta en una unidad (1 habitación), mientras se mantienen constantes las otras variables.</p></li>
<li><p><em>lstat_centered</em>: -.64 representa el cambio predicho en medv cuando lstat_centered aumenta en una unidad, mientras se mantienen constantes las otras variables.</p></li>
<li><p><em>chas</em>: 4.12 representa el cambio predicho en medv cuando chas aumenta en una unidad, mientras se mantienen constantes las otras variables.</p></li>
</ul>
</div>
<div id="interpretación-del-intercept-y-los-coeficientes-beta-para-un-modelo-con-predictores-binarios-y-continuos-con-interacciones" class="section level3" number="3.14.3">
<h3><span class="header-section-number">3.14.3</span> Interpretación del <em>intercept</em> y los coeficientes <span class="math inline">\(\beta\)</span> para un modelo con predictores binarios y continuos, con interacciones</h3>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="regresión-lineal.html#cb173-1"></a><span class="kw">display</span>(<span class="kw">lm</span>(medv <span class="op">~</span><span class="st"> </span>rm_centered<span class="op">*</span><span class="st"> </span>chas <span class="op">+</span><span class="st"> </span>lstat_centered, <span class="dt">data =</span> Boston))</span></code></pre></div>
<pre><code>## lm(formula = medv ~ rm_centered * chas + lstat_centered, data = Boston)
##                  coef.est coef.se
## (Intercept)      22.25     0.25  
## rm_centered       4.98     0.46  
## chas              4.17     0.99  
## lstat_centered   -0.64     0.04  
## rm_centered:chas -0.22     1.13  
## ---
## n = 506, k = 5
## residual sd = 5.45, R-Squared = 0.65</code></pre>
<p>Recordemos que la visualización de datos en estadística es muy necesaria!</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="regresión-lineal.html#cb175-1"></a><span class="kw">ggplot</span>(Boston, <span class="kw">aes</span>(rm_centered, medv, <span class="dt">col=</span> <span class="kw">factor</span>(chas))) <span class="op">+</span><span class="st"> </span></span>
<span id="cb175-2"><a href="regresión-lineal.html#cb175-2"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb175-3"><a href="regresión-lineal.html#cb175-3"></a><span class="st">  </span><span class="kw">stat_smooth</span>(<span class="dt">method=</span><span class="st">&quot;lm&quot;</span>, <span class="dt">se=</span>F)<span class="op">+</span></span>
<span id="cb175-4"><a href="regresión-lineal.html#cb175-4"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;medv ~ rm_centered según chas&quot;</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-53-1.png" width="672" /></p>
<p>Estas rectas de regresión son prácticamente paralelas, lo que indica que no hay interacción. Este resultado se confirma por el hecho de que el valor p para rm_centered: chas es .85 (p&gt; .05).</p>
<ul>
<li><p><strong>intercept</strong>: 4.98 es el valor predicho de medv cuando todos los predictores son 0.</p></li>
<li><p><em>rm_centered</em>: 4.98 representa el cambio predicho en medv cuando rm_centered aumenta en una unidad (1 habitación), entre aquellas casas donde chas = 0.</p></li>
<li><p><em>chas</em>: 4.17 representa el cambio predicho en medv cuando chas aumenta en una unidad, entre hogares con habitaciones promedio (rm_centered = 0).</p></li>
<li><p><em>lstat_centered</em>: -.64 representa el cambio predicho en medv cuando lstat_centered aumenta en una unidad, mientras se mantienen constantes las otras variables.</p></li>
<li><p><em>rm_centered: chas</em>: -.21 se agrega a la pendiente de rm_centered, 4.98, para chas aumenta de 0 a 1. O, alternativamente, se agrega -.21 a la pendiente de chas, 4.17, por cada unidad adicional de rm__centrado.</p></li>
</ul>
</div>
<div id="interpretación-del-intercept-y-los-coeficientes-beta-para-un-modelo-con-predictores-continuos-con-interacciones" class="section level3" number="3.14.4">
<h3><span class="header-section-number">3.14.4</span> Interpretación del <em>intercept</em> y los coeficientes <span class="math inline">\(\beta\)</span> para un modelo con predictores continuos, con interacciones</h3>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="regresión-lineal.html#cb176-1"></a><span class="kw">display</span>(<span class="kw">lm</span>(medv <span class="op">~</span><span class="st"> </span>rm_centered <span class="op">*</span><span class="st"> </span>lstat_centered, <span class="dt">data =</span> Boston))</span></code></pre></div>
<pre><code>## lm(formula = medv ~ rm_centered * lstat_centered, data = Boston)
##                            coef.est coef.se
## (Intercept)                21.04     0.23  
## rm_centered                 3.57     0.39  
## lstat_centered             -0.85     0.04  
## rm_centered:lstat_centered -0.48     0.03  
## ---
## n = 506, k = 4
## residual sd = 4.70, R-Squared = 0.74</code></pre>
<p>Primero, visualizaremos la interacción dicotomizando lstat.</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="regresión-lineal.html#cb178-1"></a>Boston<span class="op">$</span>lstat_bin &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Boston<span class="op">$</span>lstat <span class="op">&gt;</span><span class="st"> </span><span class="kw">mean</span>(Boston<span class="op">$</span>lstat), <span class="st">&quot;above avg&quot;</span>,<span class="st">&quot;below avg&quot;</span>)</span>
<span id="cb178-2"><a href="regresión-lineal.html#cb178-2"></a><span class="kw">ggplot</span>(Boston, <span class="kw">aes</span>(rm_centered, medv, <span class="dt">col=</span> lstat_bin) ) <span class="op">+</span><span class="st"> </span></span>
<span id="cb178-3"><a href="regresión-lineal.html#cb178-3"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb178-4"><a href="regresión-lineal.html#cb178-4"></a><span class="st">  </span><span class="kw">stat_smooth</span>(<span class="dt">method=</span><span class="st">&quot;lm&quot;</span>) <span class="op">+</span></span>
<span id="cb178-5"><a href="regresión-lineal.html#cb178-5"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;medv ~ rm según lstat&quot;</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-55-1.png" width="672" /></p>
<p>El número de habitaciones en una casa claramente afecta el valor — ambas rectas de regresión son positivas. Pero esta relación positiva es más pronunciada entre los hogares con menor lstat. Aumentar la cantidad de habitaciones tiene un impacto mayor en los vecindarios más pobres que en los vecindarios más ricos.</p>
<p>A continuación, dicotomizamos rm.</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="regresión-lineal.html#cb179-1"></a>Boston<span class="op">$</span>rm_bin &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Boston<span class="op">$</span>rm <span class="op">&gt;</span><span class="st"> </span><span class="kw">mean</span>(Boston<span class="op">$</span>rm), <span class="st">&quot;above avg&quot;</span>,<span class="st">&quot;below avg&quot;</span>)</span>
<span id="cb179-2"><a href="regresión-lineal.html#cb179-2"></a><span class="kw">ggplot</span>(Boston, <span class="kw">aes</span>(lstat_centered, medv, <span class="dt">col =</span> rm_bin) ) <span class="op">+</span><span class="st"> </span></span>
<span id="cb179-3"><a href="regresión-lineal.html#cb179-3"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb179-4"><a href="regresión-lineal.html#cb179-4"></a><span class="st">  </span><span class="kw">stat_smooth</span>(<span class="dt">method=</span><span class="st">&quot;lm&quot;</span>) <span class="op">+</span></span>
<span id="cb179-5"><a href="regresión-lineal.html#cb179-5"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;medv ~ lstat según rm&quot;</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-56-1.png" width="672" /></p>
<p>El nivel socioeconómico promedio en un vecindario afecta claramente el valor de la vivienda; ambas rectas de regresión son negativas. Pero esta relación negativa es más pronunciada entre los hogares con habitaciones por encima del promedio. El nivel socioeconómico bajo (lstat aumentado) tiene un mayor impacto en el valor de las viviendas con habitaciones por encima del promedio que en las casas con habitaciones por debajo del promedio.</p>
<ul>
<li><p><strong>intercept</strong>: 21.04 es el valor predicho de medv cuando tanto rm como lstat son promedios.</p></li>
<li><p><em>rm_centered</em>: 3.57 es el cambio predicho en medv si rm_centered aumenta en 1 unidad, entre aquellos hogares donde lstat_centered es promedio (= 0).</p></li>
<li><p><em>lstat_centered</em>: .85 es el cambio predicho en medv si lstat_centered aumenta en 1 unidad, entre aquellos hogares donde rm_centered es promedio (= 0)</p></li>
<li><p><em>rm_centered: lstat_centered</em>: .48 se agrega a la pendiente de rm_centered, 3.57, por cada unidad adicional de lstat_centered. O, alternativamente, se agrega -.48 a la pendiente de lstat_centered, -.85, para cada unidad adicional de rm__centered. Podemos entender la interacción diciendo que la importancia de lstat como predictor de medv disminuye a mayor número de habitaciones y, de manera similar, que la importancia de rm como predictor de medv disminuye a niveles más altos de lstat.</p></li>
</ul>
</div>
</div>
<div id="centrado-y-escalado" class="section level2" number="3.15">
<h2><span class="header-section-number">3.15</span> Centrado y escalado</h2>
<p>Hemos visto cómo los predictores centrados pueden ayudar a la interpretación del modelo. Además de centrar, podemos <em>escalar</em> predictores, lo que hace que los coeficientes del modelo resultante sean directamente comparables (nos puede servir para discernir qué variable influye más en el resultado). La función <code>rescale ()</code> en el paquete arm automáticamente centra una variable y divide por 2 desviaciones estándar (<span class="math inline">\(\frac {x_i - \bar {x}}{2sd}\)</span>). La configuración predeterminada ignora las variables binarias. La división por 2 desviaciones estándar, en lugar de 1 (como cuando se calcula una puntuación z tradicional), hace que las variables continuas reescaladas sean comparables a las variables binarias no transformadas. Después de centrar y escalar, los coeficientes del modelo se pueden usar para evaluar los tamaños del efecto e identificar los predictores más fuertes.</p>
<p>¿Cuál es un predictor más sólido del número de ciclistas, la velocidad del viento o el año?</p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="regresión-lineal.html#cb180-1"></a><span class="kw">display</span>(<span class="kw">lm</span>(count <span class="op">~</span><span class="st"> </span>windspeed <span class="op">+</span><span class="st"> </span>year, <span class="dt">data =</span> day))</span></code></pre></div>
<pre><code>## lm(formula = count ~ windspeed + year, data = day)
##             coef.est coef.se 
## (Intercept)  4496.05   161.80
## windspeed   -5696.31   733.60
## year         2183.75   113.63
## ---
## n = 731, k = 3
## residual sd = 1535.95, R-Squared = 0.37</code></pre>
<p>Este modelo hace que parezca que windpseed es, con mucho, el predictor más fuerte: el valor absoluto del coeficiente es más de 2 veces mayor. Pero este resultado es engañoso, un artefacto de escala variable.</p>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="regresión-lineal.html#cb182-1"></a><span class="kw">display</span>(<span class="kw">lm</span>(count <span class="op">~</span><span class="st"> </span>arm<span class="op">::</span><span class="kw">rescale</span>(windspeed) <span class="op">+</span><span class="st"> </span>year, <span class="dt">data =</span> day))</span></code></pre></div>
<pre><code>## lm(formula = count ~ arm::rescale(windspeed) + year, data = day)
##                         coef.est coef.se
## (Intercept)             3410.98    80.40
## arm::rescale(windspeed) -882.90   113.70
## year                    2183.75   113.63
## ---
## n = 731, k = 3
## residual sd = 1535.95, R-Squared = 0.37</code></pre>
<p>El ajuste del modelo no ha cambiado — <span class="math inline">\(R^2\)</span> es el mismo en ambos modelos — pero el centrado y la escala nos permiten ver que el año en realidad tiene un tamaño de efecto mucho mayor que la velocidad del viento: se asocia un aumento de 1 unidad en el año con un mayor cambio en el número de pasajeros. Podríamos, de manera equivalente, usar la función <code>estandardize ()</code>, también del paquete arm, que convenientemente cambia la escala de todas las variables en un modelo a la vez.</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="regresión-lineal.html#cb184-1"></a><span class="kw">display</span>(<span class="kw">standardize</span>(<span class="kw">lm</span>(count <span class="op">~</span><span class="st"> </span>windspeed <span class="op">+</span><span class="st"> </span>year, <span class="dt">data=</span> day)))</span></code></pre></div>
<pre><code>## lm(formula = count ~ z.windspeed + c.year, data = day)
##             coef.est coef.se
## (Intercept) 4504.35    56.81
## z.windspeed -882.90   113.70
## c.year      2183.75   113.63
## ---
## n = 731, k = 3
## residual sd = 1535.95, R-Squared = 0.37</code></pre>
<p>La función <code>standardize()</code> nos advierte que la velocidad del viento es ahora un z-score (z.windspeed), y que ese año se ha centrado (c.year). La interpretación de c.year es la misma que para año: un aumento de 1 unidad (-.5 a .5) se asocia con un aumento previsto de 2183,75 pasajeros. Sin embargo, la interpretación de z.windspeed ahora es diferente, ya que un aumento de 1 unidad en z.windspeed es 2 desviaciones estándar. Por lo tanto, un aumento de 2 desviaciones estándar en la velocidad del viento (0.15) se asocia con un cambio previsto en el número de pasajeros de -882.</p>
<p>¿Por qué nos importa poder comparar coeficientes? El valor absoluto de <span class="math inline">\(\beta\)</span> es una medida de la fuerza de la relación entre un predictor y el resultado y, por lo tanto, de la importancia de ese predictor para explicar el resultado. Los valores p no ofrecen orientación sobre la <em>fuerza</em> de un predictor: un predictor estadísticamente significativo podría tener un tamaño de efecto minúsculo y prácticamente intrascendente. El valor absoluto de <span class="math inline">\(\beta\)</span> es, por tanto, una medida de importancia práctica, en oposición a la significación estadística. La significancia estadística expresa la improbabilidad de un resultado, mientras que <span class="math inline">\(\beta\)</span> representa el tamaño del efecto, cuánto esperamos que cambie el resultado como resultado de variar el predictor. Estandarizar <span class="math inline">\(\beta\)</span> nos permite interpretar el tamaño del efecto sin dejarnos engañar por diferencias arbitrarias en la escala variable.</p>
</div>
<div id="transformación-de-datos" class="section level2" number="3.16">
<h2><span class="header-section-number">3.16</span> Transformación de datos</h2>
<p>Hasta ahora hemos considerado transformaciones que no cambian el ajuste del modelo, sino que simplemente ayudan a la interpretación. A veces, sin embargo, queremos cambiar las variables para que un modelo lineal se ajuste mejor. La transformación logarítmica ampliamente utilizada es un ejemplo. Deberíamos considerar una transformación logarítmica si nuestros datos están sesgados, muestran un aumento no lineal o tienen un rango grande. Regla empírica: si una variable tiene un margen superior a dos órdenes de magnitud (x 100), la transformación logarítmica probablemente mejorará el modelo.</p>
<p>Para la transformación logarítmica usaremos el logaritmo natural, designado <span class="math inline">\(\log_e\)</span>, o <span class="math inline">\(\ln\)</span>, o, en código R: <code>log()</code>. El logaritmo natural es la función inversa de la función exponencial (y viceversa): se deshacen entre sí. Por lo tanto, estas identidades: <span class="math inline">\(e ^ {\ln (x)} = x\)</span> (si <span class="math inline">\(x&gt; 0\)</span>); <span class="math inline">\(\ln (e ^ x) = x\)</span>. Para volver a poner una variable transformada logarítmica en la escala original, simplemente exponenciamos: <span class="math inline">\(x = e ^ {\ln (x)}\)</span>. En código R: <code>x = exp (log (x)</code>). Una razón para usar registros naturales es que <em>los coeficientes en la escala logarítmica natural son, aproximadamente, interpretables como diferencias proporcionales.</em> Ejemplo: para una variable transformada logarítmicamente, un coeficiente de .06 significa que una diferencia de 1 unidad en <span class="math inline">\(x\)</span> corresponde a una diferencia aproximada del 6% en <span class="math inline">\(y\)</span>, y así sucesivamente. ¿Por qué? <code>exp (.06)</code> = 1.06, un aumento del 6% desde 1 como referencia. Sin embargo, con coeficientes más grandes, esto no funciona exactamente: <code>exp (.42)</code> = 1.52, una diferencia del 52%.</p>
<p>La población de EE. UU. De 1790 a 1970 es un ejemplo de una variable que nos gustaría transformar, ya que el aumento no es lineal y es grande:</p>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="regresión-lineal.html#cb186-1"></a><span class="kw">data</span>(<span class="st">&quot;uspop&quot;</span>)</span>
<span id="cb186-2"><a href="regresión-lineal.html#cb186-2"></a><span class="kw">plot</span>(uspop, <span class="dt">main =</span> <span class="st">&quot;US population, 1790 - 1970, en millones&quot;</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-60-1.png" width="672" /></p>
<p>Pero cuando tomamos el logaritmo de la población, el aumento es (más) lineal:</p>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="regresión-lineal.html#cb187-1"></a><span class="kw">plot</span>(<span class="kw">log</span>(uspop), <span class="dt">main =</span> <span class="st">&quot;US population (scala logartímica), 1790 - 1970, en millones&quot;</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-61-1.png" width="672" /></p>
<p>La transformación logarítmica se usa con frecuencia con precios o ingresos, ya que el extremo superior de la escala para tales variables suele ser exponencialmente mayor que el inferior. Podemos registrar los valores de las viviendas en el conjunto de datos de viviendas de Boston para mejorar el ajuste del modelo.</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="regresión-lineal.html#cb188-1"></a>Boston <span class="op">%&gt;%</span></span>
<span id="cb188-2"><a href="regresión-lineal.html#cb188-2"></a><span class="st">  </span><span class="kw">arrange</span>(medv) <span class="op">%&gt;%</span></span>
<span id="cb188-3"><a href="regresión-lineal.html#cb188-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">observations =</span> <span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(medv)),</span>
<span id="cb188-4"><a href="regresión-lineal.html#cb188-4"></a>         <span class="dt">log_medv =</span> <span class="kw">log</span>(medv)) <span class="op">%&gt;%</span></span>
<span id="cb188-5"><a href="regresión-lineal.html#cb188-5"></a><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">select</span>(medv, log_medv, observations) <span class="op">%&gt;%</span></span>
<span id="cb188-6"><a href="regresión-lineal.html#cb188-6"></a><span class="st">  </span><span class="kw">gather</span>(type, home_value, <span class="op">-</span>observations) <span class="op">%&gt;%</span></span>
<span id="cb188-7"><a href="regresión-lineal.html#cb188-7"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(observations, home_value)) <span class="op">+</span></span>
<span id="cb188-8"><a href="regresión-lineal.html#cb188-8"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb188-9"><a href="regresión-lineal.html#cb188-9"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>type, <span class="dt">scales =</span> <span class="st">&quot;free_y&quot;</span>)<span class="op">+</span></span>
<span id="cb188-10"><a href="regresión-lineal.html#cb188-10"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Comparación de log(medv) y medv&quot;</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-62-1.png" width="672" /></p>
<p>Tomar el logaritmo de medv no ha desplazado medv hacia la linealidad tanto como podríamos haber esperado. No obstante, ¿mejorará el ajuste?</p>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="regresión-lineal.html#cb189-1"></a><span class="kw">display</span>(<span class="kw">standardize</span>(<span class="kw">lm</span>(medv <span class="op">~</span><span class="st"> </span>rm <span class="op">+</span><span class="st"> </span>lstat, <span class="dt">data =</span> Boston)))</span></code></pre></div>
<pre><code>## lm(formula = medv ~ z.rm + z.lstat, data = Boston)
##             coef.est coef.se
## (Intercept) 22.53     0.25  
## z.rm         7.16     0.62  
## z.lstat     -9.17     0.62  
## ---
## n = 506, k = 3
## residual sd = 5.54, R-Squared = 0.64</code></pre>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="regresión-lineal.html#cb191-1"></a><span class="kw">display</span>(<span class="kw">standardize</span>(<span class="kw">lm</span>(<span class="kw">log</span>(medv) <span class="op">~</span><span class="st"> </span>rm <span class="op">+</span><span class="st"> </span>lstat, <span class="dt">data =</span> Boston)))</span></code></pre></div>
<pre><code>## lm(formula = log(medv) ~ z.rm + z.lstat, data = Boston)
##             coef.est coef.se
## (Intercept)  3.03     0.01  
## z.rm         0.18     0.03  
## z.lstat     -0.55     0.03  
## ---
## n = 506, k = 3
## residual sd = 0.23, R-Squared = 0.68</code></pre>
<p>Un poco. <span class="math inline">\(R^2\)</span> ha mejorado de .64 a .68.</p>
<p>Se debe tener cuidado al interpretar un modelo transformado logarítmicamente.</p>
<ul>
<li><p><strong>intercept</strong>: 3.03 representa el valor log (medv) predicho cuando tanto rm como lstat son promedios (ya que ambas variables se han centrado y escalado). Para volver a poner esto en la escala original, exponencial: $e ^ {3.03} $= 20.7. Por lo tanto, el medv predicho del modelo para hogares con lstat y rm promedio en dólares es $2.07^{4}.</p></li>
<li><p><em>z.rm</em>: .18 o 18% representa el cambio porcentual previsto en medv asociado con un aumento de dos desviaciones estándar en rm (1.41).</p></li>
<li><p><em>z.lstat</em>: .55 o 55% representa el cambio porcentual previsto en medv asociado con un aumento de dos desviaciones estándar en lstat (14.28).</p></li>
<li><p><em>residual se</em>: .23 es la desviación estándar de los residuos registrados. Para volver a poner esto en la escala original, exponencial: $e ^ .23 $= $1.2586.</p></li>
</ul>
<p>¿Y si tanto el resultado como el predictor se transforman logarítmicamente?</p>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb193-1"><a href="regresión-lineal.html#cb193-1"></a><span class="kw">display</span>(<span class="kw">lm</span>(<span class="kw">log</span>(medv) <span class="op">~</span><span class="st"> </span><span class="kw">log</span>(lstat), <span class="dt">data =</span> Boston))</span></code></pre></div>
<pre><code>## lm(formula = log(medv) ~ log(lstat), data = Boston)
##             coef.est coef.se
## (Intercept)  4.36     0.04  
## log(lstat)  -0.56     0.02  
## ---
## n = 506, k = 2
## residual sd = 0.23, R-Squared = 0.68</code></pre>
<p>La transformación del registro de un predictor, así como el resultado, es una táctica perfectamente razonable si cree que la no normalidad en ambos podría estar contribuyendo a gráficos residuales problemáticos. En este caso, <span class="math inline">\(R^2\)</span> no ha cambiado, lo que sugiere que la transformación logarítmica de lstat no es necesaria.</p>
<ul>
<li><p><strong>intercept</strong>: log (medv) es 4.36 cuando lstat = 1. (Tenga en cuenta que no podemos centrar lstat en este caso porque no podemos tomar el logaritmo de un número negativo).</p></li>
<li><p><em>log (lstat)</em>: -.56 o -56% representa el cambio porcentual previsto en medv asociado con un aumento del 1% en lstat.</p></li>
</ul>
</div>
<div id="colinealidad" class="section level2" number="3.17">
<h2><span class="header-section-number">3.17</span> Colinealidad</h2>
<p>La colinealidad ocurre cuando dos variables predictoras están fuertemente correlacionadas entre sí. Si bien no es un supuesto de regresión per se, la colinealidad puede afectar la precisión de los coeficientes, así como inflar los errores estándar. La colinealidad es menos preocupante cuando solo nos interesa la predicción.</p>
<p>Un buen ejemplo es la temperatura y la temperatura percibida, atemp, en los datos de la bicicleta. Estas medidas de temperatura están muy cerca:</p>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb195-1"><a href="regresión-lineal.html#cb195-1"></a><span class="kw">ggplot</span>(day, <span class="kw">aes</span>(temperature)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb195-2"><a href="regresión-lineal.html#cb195-2"></a><span class="st">  </span><span class="kw">geom_density</span>()<span class="op">+</span></span>
<span id="cb195-3"><a href="regresión-lineal.html#cb195-3"></a><span class="st">  </span><span class="kw">geom_density</span>(<span class="kw">aes</span>(atemp), <span class="dt">col=</span> <span class="st">&quot;red&quot;</span>) <span class="op">+</span></span>
<span id="cb195-4"><a href="regresión-lineal.html#cb195-4"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Densidad de la temperatura (black) vs. la temperatura percibida (red)&quot;</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-65-1.png" width="672" /></p>
<p>Ahora compare el modelo de uso de la bicicleta con solo la temperatura como predictor con el modelo con temperatura y temperatura. Observe lo que sucede con los errores estándar y los coeficientes:</p>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="regresión-lineal.html#cb196-1"></a><span class="kw">display</span>(<span class="kw">standardize</span>(<span class="kw">lm</span>(count <span class="op">~</span><span class="st"> </span>temperature, <span class="dt">data =</span> day)))</span></code></pre></div>
<pre><code>## lm(formula = count ~ z.temperature, data = day)
##               coef.est coef.se
## (Intercept)   4504.35    55.83
## z.temperature 2431.18   111.73
## ---
## n = 731, k = 2
## residual sd = 1509.39, R-Squared = 0.39</code></pre>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb198-1"><a href="regresión-lineal.html#cb198-1"></a><span class="kw">display</span>(model &lt;-<span class="st"> </span><span class="kw">standardize</span>(<span class="kw">lm</span>(count <span class="op">~</span><span class="st"> </span>temperature <span class="op">+</span><span class="st"> </span>atemp, <span class="dt">data =</span> day)))</span></code></pre></div>
<pre><code>## lm(formula = count ~ z.temperature + z.atemp, data = day)
##               coef.est coef.se
## (Intercept)   4504.35    55.65
## z.temperature  390.34   866.32
## z.atemp       2057.91   866.32
## ---
## n = 731, k = 3
## residual sd = 1504.60, R-Squared = 0.40</code></pre>
<p>Los <span class="math inline">\(SE\)</span>s se hacen enormes y los coeficientes se vuelven poco fiables.</p>
<p>La solución práctica en el caso anterior es usar solo una de estas variables sabiendo que contienen la misma información. Pero las variables a menudo están correlacionadas. ¿Cuánta correlación está bien? El factor de inflación de la varianza (VIF) puede ayudarnos a decidir.</p>
<p>La varianza muestral estimada del coeficiente de regresión <span class="math inline">\(j\)</span>-ésimo se puede escribir como:</p>
<p><span class="math display">\[{\rm \widehat {var}} (\hat {\beta} _j) = \frac {\hat {\sigma} ^ 2} {(n-1) s_j ^ 2} \cdot \frac {1} {1-R_j ^ 2}\]</span></p>
<p>donde <span class="math inline">\(\hat {\sigma} ^ 2\)</span> es la varianza del error estimada, <span class="math inline">\(s_j ^ 2\)</span> es la varianza muestral de $x_j $y <span class="math inline">\(\frac {1} {1-R_j ^ 2}\)</span> es el factor de inflación de la varianza o <span class="math inline">\(VIF_j\)</span>.</p>
<p>El término <span class="math inline">\(R_j ^ 2\)</span> es el <span class="math inline">\(R ^ 2\)</span> de un modelo de regresión lineal en el que el predictor <span class="math inline">\(X_j\)</span> se utiliza como variable de respuesta y todas las demás covariables como variables explicativas. Un <span class="math inline">\(R ^ 2\)</span> alto en dicho modelo significa que la mayor parte de la variación en el predictor <span class="math inline">\(X_j\)</span> se explica por todas las demás covariables, lo que significa que hay colinealidad. Esto infla los errores estándar, ensanchando los IC y disminuyendo la probabilidad de detectar un efecto. Para evaluar la colinealidad entre los predictores, use <code>vif ()</code> del paquete car:</p>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb200-1"><a href="regresión-lineal.html#cb200-1"></a><span class="kw">cor</span>(day<span class="op">$</span>temperature, day<span class="op">$</span>atemp)</span></code></pre></div>
<pre><code>## [1] 0.9917016</code></pre>
<p>Estas variables están casi perfectamente correlacionadas.</p>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb202-1"><a href="regresión-lineal.html#cb202-1"></a>car<span class="op">::</span><span class="kw">vif</span>(model)</span></code></pre></div>
<pre><code>## z.temperature       z.atemp 
##      60.50328      60.50328</code></pre>
<p>Este resultado significa que los errores estándar son aproximadamente <span class="math inline">\(\sqrt {60.5}\)</span>= 7.8 más grandes de lo que serían sin la otra variable. El punto críto es ver que valor se considera grande … Hay autores que consideran un umbral de VIF de 4 (John Fox) o 5-10 (Hastie &amp; Tibshirani) para eliminar una variable. Lo que hay que hacer es decir cuál es nuestro criterio y dejar que el lector considere si es mucho o poco.</p>
</div>
<div id="valores-atípicos" class="section level2" number="3.18">
<h2><span class="header-section-number">3.18</span> Valores atípicos</h2>
<p>Los valores atípicos pueden afectar el ajuste de un modelo de regresión. Es mejor no eliminarlos (al menos no al principio) sino comprenderlos. (Por supuesto, algunos valores extremos podrían ser errores de codificación, en cuyo caso querrá eliminarlos). Recuerde que los valores atípicos son principalmente una preocupación <em>después</em> de que el modelo se ha ajustado, en cuyo caso aparecen entre los residuos. Los predictores incluidos en el modelo pueden ocuparse de las observaciones que aparecen como valores atípicos en el análisis univariado o bivariado. Como ejemplo, consideremos un conjunto de datos incluido en la librería ISLR, Hitters, que contiene información de las estadísticas de rendimiento y los salarios de los jugadores de béisbol de las grandes ligas en la temporada de 1986. Creemos un modelo para predecir el salario usando el número de turnos al bate, hits, años en la liga, home-runs, carreras impulsadas, bases por bolas y asistencias.</p>
<div class="sourceCode" id="cb204"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb204-1"><a href="regresión-lineal.html#cb204-1"></a><span class="kw">library</span>(ISLR)</span>
<span id="cb204-2"><a href="regresión-lineal.html#cb204-2"></a><span class="kw">data</span>(Hitters)</span>
<span id="cb204-3"><a href="regresión-lineal.html#cb204-3"></a></span>
<span id="cb204-4"><a href="regresión-lineal.html#cb204-4"></a><span class="kw">display</span>(<span class="kw">standardize</span>(m &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(Salary) <span class="op">~</span><span class="st"> </span>AtBat <span class="op">+</span><span class="st"> </span>Hits <span class="op">+</span></span>
<span id="cb204-5"><a href="regresión-lineal.html#cb204-5"></a><span class="st">                              </span>Years <span class="op">+</span><span class="st"> </span>HmRun <span class="op">+</span><span class="st"> </span>RBI <span class="op">+</span><span class="st"> </span>Walks <span class="op">+</span></span>
<span id="cb204-6"><a href="regresión-lineal.html#cb204-6"></a><span class="st">                              </span>Assists, <span class="dt">data =</span> Hitters)))</span></code></pre></div>
<pre><code>## lm(formula = log(Salary) ~ z.AtBat + z.Hits + z.Years + z.HmRun + 
##     z.RBI + z.Walks + z.Assists, data = Hitters)
##             coef.est coef.se
## (Intercept)  5.88     0.04  
## z.AtBat     -0.81     0.34  
## z.Hits       1.29     0.32  
## z.Years      0.91     0.08  
## z.HmRun      0.11     0.17  
## z.RBI        0.02     0.23  
## z.Walks      0.34     0.10  
## z.Assists    0.03     0.09  
## ---
## n = 263, k = 8
## residual sd = 0.63, R-Squared = 0.51</code></pre>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb206-1"><a href="regresión-lineal.html#cb206-1"></a><span class="kw">plot</span>(m, <span class="dt">which=</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-69-1.png" width="672" /></p>
<p>Los salarios de tres jugadores — Mike Schmidt, Terry Kennedy y Steve Sax — claramente no están muy bien explicados por este modelo. El modelo predice que a los dos primeros se les debería pagar menos y que al tercero se les debería pagar más. Podemos obtener otra perspectiva de estas observaciones utilizando una métrica llamada “distancia de Cook”, que es una medida de influencia de uso común.</p>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="regresión-lineal.html#cb207-1"></a><span class="kw">plot</span>(m, <span class="dt">which=</span><span class="dv">4</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-70-1.png" width="672" /></p>
<p>Un punto influyente es aquel que, si se elimina de los datos, cambiaría significativamente el ajuste. El punto podría ser un valor atípico o tener un alto apalancamiento. La distancia de Cook&gt; 1 se usa a menudo como un umbral aproximado para identificar puntos influyentes. La diferencia de Cook para estos tres jugadores es menor que 1, pero al mismo tiempo no están bien explicados por el modelo. ¿Qué debemos hacer?</p>
<p>Primero, pensar en los datos. Hablar con expertos, generalmente con los investigadores que han diseñado el estudio y han recogido la información para que nos ayuden a comprender si dichos valores tienen sentido (quizás podrían ser errores de medida o de entrada de datos).</p>
<ul>
<li><p>En el caso del conjunto de datos de los bateadores, tenemos datos de solo un año, 1986 — puede haber mucha variabilidad — Pero tenemos estadísticas de carrera en el conjunto de datos. ¿Quizás deberíamos usar estadísticas de carrera promedio como predictores en su lugar?</p></li>
<li><p>Podríamos hacer una investigación histórica para averiguar por qué a Schmidt y Kennedy se les pagó tanto en relación con sus prestaciones (¿quizás se lesionaron durante la temporada?) Y por qué a Sax se le pagó comparativamente poco. Dicha investigación podría ayudarnos a identificar información explicativa adicional que podría codificarse en variables e incluirse en el modelo y que podría mejorar el ajuste del modelo.</p></li>
<li><p>¿Hay alguna característica compartida por estos valores atípicos que podamos codificar en una variable e incluir como predictor en la regresión?</p></li>
</ul>
<p>En pocas palabras: no existen reglas estrictas y rápidas sobre qué hacer con los valores atípicos (o incluso lo que cuenta como un valor atípico). Usamos estas herramientas de diagnóstico, las gráficas de distancia y residuales de Cook, para comprender mejor nuestros datos y diseñar enfoques que tengan sentido en el contexto de nuestro proyecto. NOTA IMPORTANTE: Debemos tener mucho cuidado al descartar datos (a parte que no es una buena conducta). Tan sólo podríamos descartarlos si estuviéramos 100% seguros que son errores.</p>

</div>
</div>








<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>De Fox, John (2016). <em>Regresión aplicada y modelos lineales generalizados.</em> Sage: Los Ángeles.<a href="regresión-lineal.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>La gente usa 2 en lugar de 1,96 para un cálculo rápido. De ahí lo de “más menos 2 veces el error estándar”<a href="regresión-lineal.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>De Gelman y Hill (2007). <em>Análisis de datos mediante regresión y modelos jerárquicos / multinivel</em>. Cambridge: Cambridge UP.<a href="regresión-lineal.html#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="tidyverse.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/02-regresion_lineal.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
