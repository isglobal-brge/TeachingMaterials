---
title: "Tidyverse"
subtitle: "R for Data Science"
author: |
  | Juan R Gonzalez
  | juanr.gonzalez@isglobal.org
institute: |
  | BRGE - Bioinformatics Research Group in Epidemiology
  | ISGlobal - Barcelona Institute for Global Health
  | http://brge.isglobal.org
output:
  beamer_presentation:
    toc: false
    slide_level: 2
    includes:
      in_header: header.tex  
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment="", message=FALSE, warning=FALSE, cache=TRUE, fig.width = 4, fig.height = 4)
options(width=80)
```

## Preliminaries

```{r install, eval=FALSE}
install.packages(c("tidyverse",
                   "nycflights13"))
```

I assume you are familiar with:

* R
* RStudio
* RMarkdown



## Introduction
* Data science is an exciting discipline that allows you to turn raw data into understanding, insight, and knowledge. 
* R can help you learn the most important tools that will allow you to do data science.
* Data science is a huge field, and this lectures aim to introduce you on it


# Tidyverse

## Introduction

![Tidyverse](figures/tidyverse.png)

```{r install_tidyverse, eval=FALSE}
install.packages("tidyverse")
```

## What you will learn

![Data science](figures/data-science.png)

**Tidying** data means storing it in a consistent form that matches the semantics of the dataset with the way it is stored


# Data wragling

## Data wrangling

![Data wrangling](figures/data-science-wrangle.png)

## Data wrangling
    
* In **tibbles**, the counterpart of data.frames in tidyverse. 
* In **data import** you get data from disk into R focusing on plain-text rectangular formats (other types are possible)
* In **tidy** data,  a consistent way of storing your data that makes transformation, visualisation, and modelling easier.

## Data Wrangling

Also encompasses data transformation (not covered here) that facilitates:

*  Relational data will give you tools for working with multiple interrelated datasets.

* Strings will introduce regular expressions, a powerful tool for manipulating strings.

* Factors are how R stores categorical data. They are used when a variable has a fixed set of possible values, or when you want to use a non-alphabetical ordering of a string.

* Dates and times will give you the key tools for working with dates and date-times.

## Tibbles

You can learn more by executing vignette("tibble")

```{r load_tydiverse}
library(tidyverse)
```

* Creating tibles

```{r iris}
head(iris)
```

```{r create_tible}
iris.tib <- as_tibble(iris)
iris.tib
```

---

A new tibble can be created by (data is recycled):

```{r create_new}
tibble(
  x = 1:5, 
  y = 1, 
  z = x ^ 2 + y
)
```

NOTE: It never changes the type of data (i.e. character to factor)

--- 
  
Tibbles have a refined print method that shows only the first 10 rows, and all the columns that fit on screen. This can be changed 

```{r change_print}
print(iris.tib, n = 10, width = Inf)
``` 

---

```{r change_print2}
print(iris.tib, n = 10, width = 25)
``` 

---

* Subsetting

```{r subsetting}
df <- tibble(
  x = runif(5),
  y = rnorm(5)
)

# Extract by name
df$x
df[["x"]]
# Extract by position
df[[1]]
```

## Exercises (tibbles)


1. How can you know whether an object is a tibble? (Hint: try printing mtcars, which is a regular data frame).

2. If you have the name of a variable stored in an object, e.g. var <- "mpg", how can you extract the reference variable from a tibble?

3. What option controls how many additional column names are printed at the footer of a tibble?

4. Practice creating new variables in the following data frame

``` 
         tbl <- tibble(
                  age = c(14, 18, 22, 12, 16, 19, 21, 24),
                  chol = c(172, 180, 185, 170, 175, 188, 190, 192),
                  sex = c("male", "male", "female", "female", "female", "male", "male", "male"
                  )
                )
```

by:
    + Extracting the variable called sex.
    + Plotting a scatterplot of age vs chol.
    + Creating a new column called chol2 which is chol to the power of 2.
    + Rename the columns to one, two and three.



# Data import

## Data import

The key package is *readr*

- _read_csv()_ reads comma delimited files, _read_csv2()_ reads semicolon separated files (common in countries where , is used as the decimal place), _read_tsv()_ reads tab delimited files, and _read_delim()_ reads in files with any delimiter.

-  _read_fwf()_ reads fixed width files. You can specify fields either by their widths with _fwf_widths()_ or their position with _fwf_positions()_. _read_table()_ reads a common variation of fixed width files where columns are separated by white space.

- _read_log()_ reads Apache style log files. (But also check out webreadr which is built on top of _read_log()_ and provides many more helpful tools.)

## Comparison with base R

- They are typically much faster (~10x) than their base equivalents. Long running jobs have a progress bar, so you can see what’s happening. If you’re looking for raw speed, try data.table::fread(). It doesn’t fit quite so well into the tidyverse, but it can be quite a bit faster.

- They produce tibbles, they don’t convert character vectors to factors, use row names, or munge the column names. These are common sources of frustration with the base R functions [Hadley statement!].

- They are more reproducible. Base R functions inherit some behaviour from your operating system and environment variables, so import code that works on your computer might not work on someone else’s.

---

```{r compare}
library(readr)
system.time(dd1 <- read.delim("../../data/genome.txt"))
system.time(dd2 <- read_delim("../../data/genome.txt", 
                              delim="\t"))
dim(dd2)
```
---

```{r compare_read}
head(dd1)
dd2
```

# Data transformation

## Data transformation
* It is rare that you get the data in exactly the right form you need. 
* Often you’ll need to create some new variables or summaries.
* Or maybe you just want to rename the variables or reorder the observations in order to make the data a little easier to work with. 

---

Let us illustrate how to manage available data using NYC fligths database. `nycflights13::flights` data frame contains all 336,776 flights that departed from New York City in 2013. The data comes from the US Bureau of Transportation Statistics, and is documented in `?flights`.

```{r load_data}
library(nycflights13)
library(tidyverse)
```

----

```{r show_flights}
flights
```

## dlpyr basics

* Pick observations by their values: `filter()`.
* Reorder the rows: `arrange()`.
* Pick variables by their names: `select()`.
* Create new variables with functions of existing variables: `mutate()`.
* Collapse many values down to a single summary: `summarise()`.

---- 

All verbs work similarly:

- The first argument is a data frame.

- The subsequent arguments describe what to do with the data frame, using the variable names (without quotes).

- The result is a new data frame.

## Filter rows

```{r filter}
jan1 <- filter(flights, month == 1, day == 1)
```

R either prints out the results, or saves them to a variable. If you want to do both, you can wrap the assignment in parentheses:

```{r filter2}
(jan1 <- filter(flights, month == 1, day == 1))
```
## Logical filtering

![boolean operations](figures/transform-logical.png)

--- 

```{r example_boolean1}
filter(flights, month == 11 | month == 12)
```

---

```{r example_boolean2}
filter(flights, !(arr_delay > 120 | dep_delay > 120))
```


## Arrange rows

```{r arrange}
arrange(flights, year, month, day)
```

----

```{r arrange2}
arrange(flights, desc(dep_delay))
```
NOTE: missing values are located at the end

## Select columns

```{r select}
select(flights, year, month, day)
```

--- 

```{r select2}
select(flights, year:day)
```

---

```{r select3}
select(flights, -(year:day))
```

---

There are a number of helper functions you can use within select():

*  `starts_with("abc")`: matches names that begin with “abc”.

* `ends_with("xyz")`: matches names that end with “xyz”.

* `contains("ijk")`: matches names that contain “ijk”.

* `matches("(.)\\1")`: selects variables that match a regular expression. This one matches any variables that contain repeated characters. You’ll learn more about regular expressions in `strings`.

* `num_range("x", 1:3)`: matches x1, x2 and x3.

## Add new variables

```{r add_var}
flights_sml <- select(flights, 
  year:day, 
  ends_with("delay"), 
  distance, 
  air_time
)
mutate(flights_sml,
  gain = dep_delay - arr_delay,
  speed = distance / air_time * 60
)
```

----

If you only want to keep the new variables, use `transmute()`:

```{r transmute}
transmute(flights,
  gain = dep_delay - arr_delay,
  hours = air_time / 60,
  gain_per_hour = gain / hours
)
```

## Grouped summaries

```{r summary}
summarise(flights, delay = mean(dep_delay, na.rm = TRUE))
```

--- 

```{r summary2}
by_day <- group_by(flights, year, month, day)
summarise(by_day, delay = mean(dep_delay, na.rm = TRUE))
```

## The pipe `%>%`

Imagine that we want to explore the relationship between the distance and average delay for each location. The steps are:

There are three steps to prepare this data:

* Group flights by destination.

* Summarise to compute distance, average delay, and number of flights.

* Filter to remove noisy points and Honolulu airport, which is almost twice as far away as the next closest airport.


---

Using what you know about `dplyr`, you might write code like this:

```{r pipe}
by_dest <- group_by(flights, dest)
delay <- summarise(by_dest,
  count = n(),
  dist = mean(distance, na.rm = TRUE),
  delay = mean(arr_delay, na.rm = TRUE)
)
delay <- filter(delay, count > 20, dest != "HNL")
delay
```

----

```{r plot_pipe}
ggplot(data = delay, mapping = aes(x = dist, y = delay)) +
  geom_point(aes(size = count), alpha = 1/3) +
  geom_smooth(se = FALSE)
```

----

```{r pipe_use}
delays <- flights %>% 
  group_by(dest) %>% 
  summarise(
    count = n(),
    dist = mean(distance, na.rm = TRUE),
    delay = mean(arr_delay, na.rm = TRUE)
  ) %>% 
  filter(count > 20, dest != "HNL")
delays
```

## Group by different variables

```{r group_var}
flights %>%
  group_by(year, month, day) %>% 
  summarise(
    avg_delay1 = mean(arr_delay, na.rm=TRUE),
    avg_delay2 = mean(arr_delay[arr_delay > 0], na.rm=TRUE) 
  )
```

## Useful summary functions

* `count()`
* `mean()`
* `median()`
* `min()`
* `max()`
* `quantile(x, 0.25)`
* `IQR()`
* `mad()`


## Exercises (data transform)

1. Using `flights` dataset, find all flights that
     - Had an arrival delay of two or more hours
     - Flew to Houston (IAH or HOU)
          - Arrived more than two hours late, but didn’t leave late
     - Were delayed by at least an hour, but made up over 30 minutes in flight
     
2. Another useful dplyr filtering helper is between(). What does it do? Can you use it to simplify the code needed to answer the previous challenges?

3. Sort flights to find the fastest flights.

4. Create a new data frame having variables with the `dep` string.

5. Create a new data frame having the hour and minute of depature (Hint: information is in the variable `dep_time` with format HHMM or HMM. Use `%/%` or `%%` when appropriate) 

6. Create a summary of each airline (variable `carrier`) describing the total number of flights, the average, median, IQR and variance delays. Which is the best airline in terms of those summary statistics?


## Session info

```{r}
sessionInfo()
```

